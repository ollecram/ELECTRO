\chapter{Griffiths -- Vector Analysis}
\label{ch:Griffiths_00} 

\section{Vector Algebra}
For the sake of fixing notation, let $\{\vu{i}, \vu{j}, \vu{k}\}$ be an \textit{orthonormal} basis of unit vectors in three-dimensional space, and the following a list of all possible scalar products of basis vectors. 
  
\begin{equation}
\begin{aligned} 
\vu{i}\vdot \vu{i} &= \vu{j} \vdot \vu{j} = \vu{k} \vdot \vu{k} = 1 \\ 
\vu{i} \vdot \vu{j} &= \vu{j} \vdot \vu{k} = \vu{k} \vdot \vu{i} = 0 
\label{eq:basis_dot_products}
\end{aligned}
\end{equation}

For a \textit{right-handed} basis, the following is a list of all possible \textit{vector} products of basis vectors.
  
\begin{equation}
\begin{aligned} 
\vu{i} \cross \vu{i} &= \vu{j} \cross \vu{j} = \vu{k} \cross \vu{k} = 0 \\ 
\vu{i} \cross \vu{j} &= \vu{k} = - \vu{j} \cross \vu{i}\\
\vu{j} \cross \vu{k} &= \vu{i} = - \vu{k} \cross \vu{j}\\
\vu{k} \cross \vu{i} &= \vu{j} = - \vu{i} \cross \vu{k}
\label{eq:basis_vector_products}
\end{aligned}
\end{equation}

Any vector $\vb{A}$ can be resolved into a linear combination of the basis vectors
\begin{equation*}
\vb{A} = A_x \, \vu{i} + A_y \, \vu{j} + A_z \, \vu{k} 
\end{equation*}

where $A_x, A_y, A_z$ are the vector \textit{components} in that basis. 

When the basis is assumed, a list of comma separated numbers within parentheses indicate the components of a vector in that basis: 

\begin{equation*}
(A_x, A_y, A_z) \equiv \vb{A} = A_x  \vu{i} + A_y  \vu{j} + A_z  \vu{k} 
\end{equation*}

The \textit{dot} product of two vectors is the \textit{scalar}  
\begin{equation}
\vb{A} \vdot \vb{B} = A_x B_x + A_y B_y + A_z B_z 
\label{eq:dot_product}
\end{equation}

The \textit{cross} product of two vectors is the \textit{vector}  
\begin{equation}
\begin{aligned}
\vb{A} \cross \vb{B} &= (A_y B_z - A_z B_y)\, \vu{i} \\
                     &+ (A_z B_x - A_x B_z)\, \vu{j} \\
                     &+ (A_x B_y - A_y B_x)\, \vu{k}
\end{aligned}
\label{eq:cross_product}
\end{equation}

A useful mnemonic expression equivalent to \ref{eq:cross_product} is the \textit{determinant} 
\begin{equation*}
\mqty| \vu{i} &  \vu{j} &  \vu{k} \\ 
A_x & A_y & A_z \\
B_x & B_y &z|
\end{equation*}


It follows from \ref{eq:basis_vector_products} that for any two vectors $\vb{A}$ and $\vb{B}$ 
\begin{equation}
\vb{A} \cross \vb{B} = - \vb{B} \cross \vb{A}
\label{eq:cross_product_reflection}
\end{equation}

\subsection{Triple products}
Since the cross product of two vector is itself a vector, it can be dotted or crossed with a third vector to form a \textit{triple} product. 

\subsubsection{Scalar triple product}
The norm of $\vb{A} \vdot (\vb{B} \cross \vb{C})$ is the volume of a parallelepiped generated by $\vb{A}$, $\vb{B}$ and $\vb{C}$, since $\abs{ \vb{B} \cross \vb{C} }$ is the area of the base, while the dot product multiplies that number by the length of the projection of $\vb{A}$ orthogonal to the area. The choice of which vectors make the base of the parallelepiped is arbitrary, while the volume is independent of that choice, therefore: 

\begin{equation}
\vb{A} \vdot (\vb{B} \cross \vb{C}) = \vb{B} \vdot (\vb{C} \cross \vb{A}) = \vb{C} \vdot (\vb{A} \cross \vb{B})
\label{eq:scalar_triple_product}
\end{equation}

When $\vb{A}$, $\vb{B}$ and $\vb{C}$ form a \textit{right-handed} [\textit{left-handed}] basis --not necessarily orthonormal-- the above forms evaluate to the same \textit{positive} [\textit{negative}] value.  

Because of \ref{eq:cross_product_reflection} the following three extra forms are also equivalent among each other but they evaluate to the \textit{opposite} value, thus yielding a \textit{negative} [\textit{positive}] value when $\vb{A}$, $\vb{B}$ and $\vb{C}$ form a \textit{right-handed} [\textit{left-handed}] basis.

\begin{equation}
\vb{A} \vdot (\vb{C} \cross \vb{B}) = \vb{B} \vdot (\vb{A} \cross \vb{C}) = \vb{C} \vdot (\vb{B} \cross \vb{A})
\end{equation}

\subsubsection{Vector triple product}

The vector triple product can be simplified by the $\vb{BAC}\mathbf{-}\vb{CAB}$ rule:
\begin{equation}
\vb{A} \cross (\vb{B} \cross \vb{C}) = \vb{B} (\vb{A} \vdot \vb{C}) - \vb{C} (\vb{A} \vdot \vb{B})
\label{eq:vector_triple_product}
\end{equation}

Note that 

\begin{equation*}
(\vb{A} \cross \vb{B}) \cross \vb{C} = - \vb{C} \cross (\vb{A} \cross \vb{B}) = \vb{B} (\vb{A} \vdot \vb{C}) - \vb{A} (\vb{B} \vdot \vb{C})
\end{equation*}

so changing the grouping yields a different vector, namely

\begin{equation*}
\vb{A} \cross (\vb{B} \cross \vb{C}) - (\vb{A} \cross \vb{B}) \cross \vb{C} = \vb{A} (\vb{B} \vdot \vb{C})  - \vb{C} (\vb{A} \vdot \vb{B})
\end{equation*}

All \textit{higher} products can be similarly reduced, so that it is never necessary for an expression to contain more than one cross product in any term. For instance
\begin{equation}
\begin{aligned} 
(\vb{A} \cross \vb{B}) \vdot (\vb{C} \cross \vb{D}) &= (\vb{A} \vdot \vb{C}) (\vb{B} \vdot \vb{D}) - (\vb{A} \vdot \vb{D}) (\vb{B} \vdot \vb{C})\, ;  \\
\vb{A} \cross [\vb{B} \cross (\vb{C} \cross \vb{D})] &= \vb{B} [\vb{A} \vdot (\vb{C} \cross \vb{D})] -  (\vb{A} \vdot \vb{B})(\vb{C} \cross \vb{D})
\end{aligned}
\end{equation}

\subsubsection{Problem 1}
Prove that 
\begin{equation*}
[\vb{A} \cross (\vb{B} \cross \vb{C})] + [\vb{B} \cross (\vb{C} \cross \vb{A})] +  [\vb{C} \cross (\vb{A} \cross \vb{B})] = 0  
\end{equation*}

By applying the BAC-CAB rule to each one of the above three terms one obtains the following six   terms\footnote{Each line is obtained by the previous one by putting in each place the vector which follows -- in the $ABC$ cycle-- the one in the same place in the line above.}
\begin{equation*}
\begin{aligned} 
\vb{A} (\vb{B} \vdot \vb{C}) &- \vb{C} (\vb{A} \vdot \vb{B})\, +\\
\vb{B} (\vb{C} \vdot \vb{A}) &- \vb{A} (\vb{B} \vdot \vb{C})\, +\\
\vb{C} (\vb{A} \vdot \vb{B}) &- \vb{B} (\vb{C} \vdot \vb{A})
\end{aligned}
\end{equation*}
Each scalar product appears twice, each time it is multiplied by the remaining vector but with opposite signs.\qed. 


\section{Position, Displacement, Separation Vectors}

The vector connecting the origin $\mathcal{O}$ to the location of a point in three dimensions is called the \textbf{position vector}
\begin{equation}
\vb{r} \equiv x \, \vu{i} + y \, \vu{j} + z \, \vu{k} 
\end{equation}

Following Griffiths, we reserve the letter $\vb{r}$ for that purpose. Its magnitude 

\begin{equation}
r = \sqrt{x^2 + y^2 + z^2} 
\end{equation}

is the distance from the origin, and 

\begin{equation}
\vu{r} = \frac{\vb{r}}{r} = \frac{x \, \vu{i} + y \, \vu{j} + z \, \vu{k}}{\sqrt{x^2 + y^2 + z^2}} 
\end{equation}

is a unit vector pointing radially outward. Following Griffiths, the \textbf{infinitesimal displacement vector} from $(x, y, z)$ to $(x+dx, y+dy, z+dz)$ is denoted as $d\vb{l}$

\begin{equation}
d\vb{l} = dx \, \vu{i} + dy \, \vu{j} + dz \, \vu{k} 
\end{equation}

Problems in electrodynamics frequently involve two points, typically a \textbf{source point}, $\vb{r}'$ and a \textbf{field point} $\vb{r}$ at which one needs to calculate the effect of the source. We use the symbol $\brcurs$ to denote the \textbf{separation vector} from the source point to the field point:

\begin{equation}
\begin{aligned}
\brcurs &\equiv \vb{r} - {\vb{r}}'\\
\rcurs &= \abs{\brcurs} = \abs{\vb{r} - {\vb{r}}'} \\
\hrcurs &= \frac{\brcurs}{\rcurs}
\end{aligned}
\label{eq:separation_vector}
\end{equation}


\section{How Vectors Transform}
Let two cartesian (orthonormal) reference frames in two dimensions, $\mathcal{F}$ and $\mathcal{F}'$, share a common origin $\mathcal{O}$. Let $\vu{i}, \vu{j}$ be unit vectors forming a basis of $\mathcal{F}$ and $\vu{i}', \vu{j}'$ some that form a basis of $\mathcal{F}'$. 

Let $\mathcal{F}'$ be rotated by the angle $\phi$ --around the origin-- with respect to $\mathcal{F}$, so that
\begin{equation}
\vu{i} \vdot \vu{i}' = \cos(\phi) = \vu{j} \vdot \vu{j}' 
\label{eq:cosphi_products}
\end{equation}
   
The components of a vector $\vb{A}$ with respect to the two frames are:

\begin{equation*}
\begin{aligned}
\vb{A} &= A_x \;\vu{i}\;  + A_y \; \vu{j}\\
\vb{A} &= A_x'\,\vu{i}'\, + A_y'\,\vu{j}'
\end{aligned}
\end{equation*}

If $\theta$ is the angle between $\vu{i}$ and $\vb{A}$, the angle between $\vu{j}$ and $\vb{A}$ is $\pi/2 - \theta$, therefore, 

\begin{equation*}
\begin{aligned}
A_x &= A \cos(\theta) \\
A_y &= A \sin(\theta) = A \cos(\pi/2 - \theta)
\end{aligned}
\end{equation*}

The angle that $\vb{A}$ forms with the axis $\vu{i}'$ of the primed frame is clearly $\theta - \phi$, therefore\footnote{Here we use the trigonometric identities $\sin(a \pm b) = \sin(a)\cos(b) \pm \cos(a)\sin(b)$ and  $\cos(a \pm b) = \cos(a)\cos(b) \mp \sin(a)\sin(b)$.}

\begin{equation*}
\begin{aligned}
A_x' &= A \cos(\theta') = A \cos(\theta - \phi) = A [\cos(\theta)\cos(\phi) + \sin(\theta)\sin(\phi)] \\
A_y' &= A \sin(\theta')\, = A \sin(\theta - \phi)\, = A [\sin(\theta)\cos(\phi) - \cos(\theta)\sin(\phi)] 
\end{aligned}
\end{equation*}
 
Substituting $A_x = A \cos(\theta)$ and $A_y = A \sin(\theta)$ in the above equations we obtain the law for transforming components to the rotated frame, namely

\begin{equation*}
\begin{aligned}
A_x' &= A_x \cos(\phi) + A_y \sin(\phi) \\
A_y' &= A_y \cos(\phi) - A_x \sin(\phi) 
\end{aligned}
\end{equation*}

In matrix notation:

\begin{equation}
\mqty[ A_x' \\
A_y'] = 
\mqty|  \cos(\phi) &  \sin(\phi) \\ 
- \sin(\phi) &  \cos(\phi) |
\mqty[ A_x \\
A_y]
\label{eq:2d_vector_transform}
\end{equation}

As expected, the matrix reduces to the identity in the limit of small $\phi$. The inverse transform is clearly obtained by changing $\phi$ into $-\phi$, thus inverting the sign of the off-diagonal terms in the above matrix. 

There is another way to derive \ref{eq:2d_vector_transform} that can be generalized to any number of dimensions. It is often presented by geometric figures with rectangular triangles to prove how the unit basis vectors $\vu{i}$ and $\vu{j}$ decompose in the rotated basis $\vu{i}'$ and $\vu{j}'$. But there is hardly the need of a figure, as  equation \ref{eq:cosphi_products} already contains half of the story.  Indeed $\vu{i}$'s decomposition as well as $\vu{j}$'s must reduce to the identity for $\phi=0$ and the sum of the two coefficients squared must be $1$. With these prescriptions\footnote{The positive sign of both the $\cos(\phi)$ terms is dictated by the need to recover the identity transformation in the limit $\phi \rightarrow 0$ . The signs of the $\sin(\phi)$ terms are determined by the relative position of $\vu{i}$ with respect to $\vu{i}'$ (lower $\vu{j}'$) and that of $\vu{j}$ with respect to $\vu{j}'$ (higher $\vu{i}'$).} 

\begin{equation}
\begin{aligned}
\vu{i} &= \cos(\phi) \vu{i}' - \sin(\phi) \vu{j}' \\
\vu{j} &= \sin(\phi) \vu{i}' + \cos(\phi) \vu{j}'
\end{aligned}
\label{eq:2d_basis_transform}
\end{equation}

and the transformation \ref{eq:2d_vector_transform} is now re-obtained from \ref{eq:2d_basis_transform}

\begin{equation*}
\begin{aligned}
\vb{A}  &= A_x \vu{i} + A_y \vu{j}\\
		&= A_x (\cos(\phi) \vu{i}' - \sin(\phi) \vu{j}') \\
		&+ A_y (\sin(\phi) \vu{i}' + \cos(\phi) \vu{j}') \\
		&= (A_x \cos(\phi) + A_y \sin(\phi)) \vu{i}' \\
		&+ (A_y \cos(\phi) - A_x \sin(\phi)) \vu{j}'
\end{aligned}
\end{equation*}
 
Generalization starts by casting \ref{eq:2d_basis_transform} into a new form, that can be readily extended to three or more dimensions. For instance, in three dimensions: 

\begin{equation*}
\begin{aligned}
\vu{i} &= \;(\vu{i} \vdot \vu{i}')\, \vu{i}' + \;(\vu{i} \vdot \vu{j}')\, \vu{j}' + \;(\vu{i} \vdot \vu{k}')\, \vu{k}'\\
\vu{j} &= \;(\vu{j} \vdot \vu{i}')\, \vu{i}' + \;(\vu{j} \vdot \vu{j}')\, \vu{j}' + \;(\vu{j} \vdot \vu{k}')\, \vu{k}'\\
\vu{k} &=   (\vu{k} \vdot \vu{i}')\, \vu{i}' +   (\vu{k} \vdot \vu{j}')\, \vu{j}' +   (\vu{k} \vdot \vu{k}')\, \vu{k}'
\end{aligned}
\end{equation*}

and finally, in a more general, dimension-independent matrix notation: 

\begin{equation}
\mqty[\vu{e_1} \\ \vu{e_2} \\ \vu{e_3}] = \mqty|  
\vu{e_1} \vdot \vu{e_1}' & \vu{e_1} \vdot \vu{e_2}' & \vu{e_1} \vdot \vu{e_3}' \\
\vu{e_2} \vdot \vu{e_1}' & \vu{e_2} \vdot \vu{e_2}' & \vu{e_2} \vdot \vu{e_3}' \\
\vu{e_3} \vdot \vu{e_1}' & \vu{e_3} \vdot \vu{e_2}' & \vu{e_3} \vdot \vu{e_3}' |
\mqty[\vu{e_1}' \\ \vu{e_2}' \\ \vu{e_3}']
= S \mqty[\vu{e_1}' \\ \vu{e_2}' \\ \vu{e_3}']
\label{eq:3d_basis_transform}
\end{equation}

The matrix $S$, whose $S_{i,j}$ component equals the scalar product $\vu{e_i} \vdot \vu{e_j}'$ transforms the primed (rotated) unit basis vectors into the unprimed ones, therefore it is the three-dimensional analogue of \ref{eq:2d_basis_transform}. Now a closer inspection at \ref{eq:2d_vector_transform} shows that the it is the \textit{transpose} of $S$ the matrix that transforms vector \textit{components} from the unprimed to the primed basis, therefore

\begin{equation}
\mqty[A_1' \\ A_2' \\ A_3'] = S^\top \mqty[A_1 \\ A_2 \\ A_3] = \mqty|
\vu{e_1} \vdot \vu{e_1}' & \vu{e_2} \vdot \vu{e_1}' & \vu{e_3} \vdot \vu{e_1}' \\
\vu{e_1} \vdot \vu{e_2}' & \vu{e_2} \vdot \vu{e_2}' & \vu{e_3} \vdot \vu{e_2}' \\
\vu{e_1} \vdot \vu{e_3}' & \vu{e_2} \vdot \vu{e_3}' & \vu{e_3} \vdot \vu{e_3}  |
\mqty[\vu{e_1}' \\ \vu{e_2}' \\ \vu{e_3}']
\label{eq:3d_components_transform}
\end{equation}

Matrices like $S$ and $S^\top$, respectively defined in \ref{eq:3d_basis_transform} and \ref{eq:3d_components_transform}, are said to be \textit{orthogonal} due to their special properties. 
Indeed each matrix \textit{row} and each matrix \textit{column} represents the components of a unit vector of one orthonormal basis with respect to another orthonormal basis, so the norm of each row and of each column must be $1$. In $n$ dimensions this amounts to $2n$ constraints, so the total number of independent parameters is $n^2-2n$. Moreover, $S$ and $S^\top$ are each other's \textit{inverse}:
\begin{equation}
S S^\top = \mqty|
\vu{e_1} \vdot \vu{e_1} & \vu{e_2} \vdot \vu{e_1}  & \vu{e_3} \vdot \vu{e_1} \\  
\vu{e_1} \vdot \vu{e_2} & \vu{e_2} \vdot \vu{e_2}  & \vu{e_3} \vdot \vu{e_2} \\  
\vu{e_1} \vdot \vu{e_3} & \vu{e_2} \vdot \vu{e_3}  & \vu{e_3} \vdot \vu{e_3} |  = S^\top S
= \mqty|
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 | = I 
\label{eq:3d_ortho_matrices}
\end{equation}

In summary, the \textit{matrices} respectively transforming the \textit{basis vectors} (\ref{eq:3d_basis_transform}) and vector \textit{components} (\ref{eq:3d_components_transform}) are simply related by \textit{transposition}. This is a direct consequence --and one of the advantages-- of working with \textit{orthonormal bases}. For transformations connecting two \textit{general bases} --where one is (or both are) not orthonormal-- the analogue of matrix $S$ --let call it $R$ --  has the same structure, although the matrix element $R_{lm} = \vb{e}_l \vdot \vb{e}_m$ is the dot product of basis vectors of arbitrary norm, whence $R$ is generally not an orthogonal matrix. In such general context, the analogue of $S^\top$ is clearly\footnote{This fact agrees with the intuitive notion that \quotes{a rotation of the basis causes an opposite rotation of vector components}.} $R^{-1}$, the \textit{inverse} of $R$. 

\subsection{Euler Angles}
We saw that \textit{one} parameter -- the $\phi$ angle -- is needed to define the transformation between two orthonormal reference systems with a common origin $\mathcal{O}$ in 2D space. In 3D space the analogous transformation generally depends on \textit{three} parameters, as it will be evident from the construction of one such transformation and from an already cited general property of orthogonal matrices. 
In the following we use lower and capital letters to distinguish axes of the two systems, and we assume the latter to have the same \textit{orientation}, namely 

\begin{equation*}
(\vu{i} \cross \vu{j}) \vdot \vu{k} = (\vu{I} \cross \vu{J}) \vdot \vu{K} = 1 
\end{equation*}

Even without a figure we can agree that the two planes respectively orthogonal to $\vu{k}$ and $\vu{K}$ are either the same plane or they intersect at a line passing through the origin, the so-called \textit{line of nodes}. We do not discuss the first case ($\vu{k} = \pm \vu{K}$) as we already know how to handle 2D transformations, so let $\theta = \acos(\vu{k} \vdot \vu{K})$ be the angle (not null) formed by the two axes. The full transformation is thus defined as the \textit{product} of three 2D rotations, each one leaving one axis unchanged, to be performed in the following order:
\begin{enumerate}
\item Around $\vu{k}$ by the angle $\phi$ formed by the axis $\vu{i}$ with the line of nodes.  
\item Around the line of nodes by the angle $\theta$ so that $\vu{k}$ goes onto $\vu{K}$.
\item Around $\vu{K}$ by the angle $\psi$ so that the line of nodes goes onto $\vu{I}$.
\end{enumerate}

$\phi$, $\theta$ and $\psi$ are called the \textit{Euler angles}. 

Let denote as $R_\phi$, $R_\theta$ and $R_\psi$ the matrices associated to the transformation from unprimed to primed coordinates under the above rotations. Each matrix has the form given in \ref{eq:2d_vector_transform} for the transformation of coordinates under a 2D rotation. Adaptation to the 3D context amonts to ensure that points located on the rotation axis be unaffected. With these prescriptions
the transformation matrix is the product $R(\phi, \theta, \psi) = R_\psi R_\theta R_\phi$ where

\begin{equation}
R_\phi = \mqty| 
\cos(\phi) &  \sin(\phi) & 0 \\
-\sin(\phi) & \cos(\phi) & 0 \\
0 & 0 & 1 | 
\end{equation}

\begin{equation}
R_\theta = \mqty|
1 & 0 & 0 \\ 
0 & \cos(\theta) &  \sin(\theta) \\
0 &-\sin(\theta) & \cos(\theta) | 
\end{equation}
  
\begin{equation}
R_\psi = \mqty| 
\cos(\psi) &  \sin(\psi) & 0 \\
-\sin(\psi) & \cos(\psi) & 0 \\
0 & 0 & 1 | 
\end{equation}

Therefore 

\begin{equation}
\begin{aligned}
& R(\phi, \theta, \psi) = R_\psi \mqty| 
\cos(\phi) & \sin(\phi) & 0 \\
-\sin(\phi)\cos(\theta) & \cos(\phi)\cos(\theta) & \sin(\theta)\\
\sin(\phi)\sin(\theta) & -\cos(\phi)\sin(\theta) & \cos(\theta) | = \\
& \mqty|
\cos{\phi}\cos{\theta} - \sin{\phi}\sin{\theta}\cos{\psi} & \cos{\phi}\sin{\theta} - \sin{\phi}\cos{\theta}\cos{\psi} & \sin{\phi} \sin{\psi}\\
\cos{\phi}\sin{\theta}\cos{\psi} - \sin{\phi}\cos{\theta} & \cos{\phi}\cos{\theta}\cos{\psi} -\sin{\phi}\sin{\theta} & \cos{\phi} \sin{\psi}\\
\sin{\theta} \sin{\psi} & -\cos{\theta}\sin{\psi} & \cos{\psi} |
\end{aligned}
\end{equation}

The determinant $\det(R(\phi, \theta, \psi))$ is equal to $1$.  
As a reminder, if $R$ is assumed to be the matrix transforming the \textit{basis} vectors, then $R^\top$ is the matrix transforming vector \textit{components} across the two bases (and viceversa).  

\section{Differential Calculus}
\subsection{Gradient}
Given $T(x, y, z)$ a scalar function of several variables the \textit{linear} part of the function increment at a given point $(x, y, z)$ is given by
\begin{equation}
\begin{aligned}
d T &= \pdv{T}{x} dx + \pdv{T}{y} dy + \pdv{T}{z} dz \\
    &= \left( \pdv{T}{x} \vu{i} + \pdv{T}{y} \vu{j} + \pdv{T}{z} \vu{k}  \right) \vdot (dx \vu{i} + dy \vu{j} + dz \vu{k}) \\
    &= \grad{T} \vdot d\vb{l}
\end{aligned}
\label{eq:gradient}
\end{equation}

where 

\begin{equation}
\grad{T} = \pdv{T}{x} \vu{i} + \pdv{T}{y} \vu{j} + \pdv{T}{z} \vu{k} 
\end{equation}

is the \textit{gradient} of $T$. 

The geometrical meaning of the gradient becomes clear by rewriting the dot product in \ref{eq:gradient} as

\begin{equation}
dT = \grad{T} \vdot d\vb{l} = \abs{\grad{T}} \abs{d\vb{l}} \cos \theta
\end{equation}

where $\theta$ is the angle between $\grad{T}$ and $d\vb{l}$. With fixed $\abs{d\vb{l}}$ the maximum of $dT$ then occurs when $\theta = 0$, therefore:

\textit{The gradient $\grad{T}$ points in the direction of maximum increase of the function $T$, while $\abs{\grad{T}}$ is the rate of increase along this maximal direction.}

The condition $\grad{T} = \vb{0}$ defines a \textit{stationary point}. It could be a maximum (a summit), a minimum (a valley), a saddle point (a pass) or a \quotes{shoulder}. 
 
\subsubsection*{Example}
Let $r \equiv \abs{\vb{r}} = \sqrt{x^2 + y^2 + z^2}$, be the \textit{magnitude} of the \textit{position} vector. Compute the gradient of different powers of $r$. 

\begin{align*} 
\grad{r} &= \frac{\vb{r}}{r} = \hat{\vb{r}} \\
\grad{r^2} &= 2 r \grad{r} = 2r \vu{r} = 2 \vb{r} \\
\grad{\left( \frac{1}{r} \right)} &= - \frac{1}{r^2} \grad{r} = - \frac{\hat{\vb{r}}}{r^2} = - \frac{\vb{r}}{r^3} 
\end{align*}

\begin{equation} 
\grad(r^n) = n r^{n-1} \grad{r} =  n r^{n-1} \vu{r} =  n r^{n-2} \vb{r}
\label{eq:grad_r_powers}
\end{equation}

\subsubsection*{Problem 13}
Let $s \equiv \abs{\vb{r} - \vb{r}'}$, be the \textit{magnitude} of the \textit{separation} vector from a \textit{fixed} point $\vb{r}'$ to the point $\vb{r}$. Show that
\begin{enumerate}[a)]
\item $\grad(s^2) = 2 \vb{s}$ 
\item $\grad(1/s) = - \vu{s}/s^2$ 
\item What is the general formula for $\grad(s^n)$?
\end{enumerate}

We note that $\vb{r}'$ is a constant term in the evaluation of derivatives, therefore
the gradient of powers of $s$ have the form \ref{eq:grad_r_powers}, therefore  

\begin{enumerate}[a)]
\item
\begin{equation*}
\grad(s^2) = 2s \vu{s} = 2 \vb{s}
\end{equation*}
\item
\begin{equation*}
\grad(1/s) = - \frac{\vu{s}}{s^2}
\end{equation*}
\item
\begin{equation}
\grad(s^n) = n s^{n-1} \grad{s} = n s^{n-1} \vu{s} 
\end{equation}
\end{enumerate}

\subsubsection*{Problem 14}
Suppose that $f$ is a function of two variables only. Show that $\grad f$ transforms as a vector under rotations. \\

The transformation of the basis vectors as in \ref{eq:2d_basis_transform} implies that the \textit{primed} basis is obtained by a rotation about the origin of the \textit{unprimed} basis by an angle $\phi$. 

It turns out (see equation \ref{eq:2d_vector_transform}) that the corresponding transformation of vector \textit{components} from the original basis to the rotated one is carried out by the following matrix  

\begin{equation*}
A(\phi) = \mqty| \cos(\phi) & \sin(\phi) \\
-\sin(\phi) & \cos(\phi)| 
\end{equation*}  

whereby a vector $\vb{r}$ takes components $(x', y')$ in the rotated basis and these can be obtained from the unprimed components $(x, y)$ that $\vb{r}$ takes in the original basis by multiplication with the matrix $A(\phi)$: 

\begin{equation*}
\mqty[ x' \\ y'] = A(\phi) \mqty[ x \\ y] 
\end{equation*}  

In the following we need to transform from the primed to the original, which can be done with the inverse of $A(\phi)$, that happens to be just $A(-\phi)$

\begin{equation*}
\mqty[ x \\ y] = A(-\phi) \mqty[ x' \\ y'] 
\end{equation*}  

Indeed, in order to obtain the gradient in the primed system, we use the following derivatives:

\begin{align*} 
\pdv{f}{x'} &= \pdv{f}{x} \pdv{x}{x'}    + \pdv{f}{y} \pdv{y}{x'}    \\
		    &= \pdv{f}{x} A(-\phi)_{1,1} + \pdv{f}{y} A(-\phi)_{2,1} \\
\pdv{f}{y'} &= \pdv{f}{x} \pdv{x}{y'}    + \pdv{f}{y} \pdv{y}{y'}    \\
		    &= \pdv{f}{x} A(-\phi)_{1,2} + \pdv{f}{y} A(-\phi)_{2,2}
\end{align*}

which nicely reduce to 

\begin{equation}
\grad'\, f = (A(-\phi))^\top \grad\, f = A(\phi) \grad\, f
\label{eq:grad_transform}
\end{equation}  
\qed

 







