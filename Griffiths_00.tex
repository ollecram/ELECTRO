\chapter{Griffiths -- Vector Analysis}
\label{ch:Griffiths_00} 

\section{Vector Algebra}
For the sake of fixing notation, let $\{\vu{i}, \vu{j}, \vu{k}\}$ be an \textit{orthonormal} basis of unit vectors in three-dimensional space, and the following a list of all possible scalar products of basis vectors. 
  
\begin{equation}
\begin{aligned} 
\vu{i}\vdot \vu{i} &= \vu{j} \vdot \vu{j} = \vu{k} \vdot \vu{k} = 1 \\ 
\vu{i} \vdot \vu{j} &= \vu{j} \vdot \vu{k} = \vu{k} \vdot \vu{i} = 0 
\label{eq:basis_dot_products}
\end{aligned}
\end{equation}

For a \textit{right-handed} basis, the following is a list of all possible \textit{vector} products of basis vectors.
  
\begin{equation}
\begin{aligned} 
\vu{i} \cross \vu{i} &= \vu{j} \cross \vu{j} = \vu{k} \cross \vu{k} = 0 \\ 
\vu{i} \cross \vu{j} &= \vu{k} = - \vu{j} \cross \vu{i}\\
\vu{j} \cross \vu{k} &= \vu{i} = - \vu{k} \cross \vu{j}\\
\vu{k} \cross \vu{i} &= \vu{j} = - \vu{i} \cross \vu{k}
\label{eq:basis_vector_products}
\end{aligned}
\end{equation}

Any vector $\vb{A}$ can be resolved into a linear combination of the basis vectors
\begin{equation*}
\vb{A} = A_x \, \vu{i} + A_y \, \vu{j} + A_z \, \vu{k} 
\end{equation*}

where $A_x, A_y, A_z$ are the vector \textit{components} in that basis. 

When the basis is assumed, a list of comma separated numbers within parentheses indicate the components of a vector in that basis: 

\begin{equation*}
(A_x, A_y, A_z) \equiv \vb{A} = A_x  \vu{i} + A_y  \vu{j} + A_z  \vu{k} 
\end{equation*}

The \textit{dot} product of two vectors is the \textit{scalar}  
\begin{equation}
\vb{A} \vdot \vb{B} = A_x B_x + A_y B_y + A_z B_z 
\label{eq:dot_product}
\end{equation}

The \textit{cross} product of two vectors is the \textit{vector}  
\begin{equation}
\begin{aligned}
\vb{A} \cross \vb{B} &= (A_y B_z - A_z B_y)\, \vu{i} \\
                     &+ (A_z B_x - A_x B_z)\, \vu{j} \\
                     &+ (A_x B_y - A_y B_x)\, \vu{k}
\end{aligned}
\label{eq:cross_product}
\end{equation}

A useful mnemonic expression equivalent to \ref{eq:cross_product} is the \textit{determinant} 
\begin{equation*}
\mqty| \vu{i} &  \vu{j} &  \vu{k} \\ 
A_x & A_y & A_z \\
B_x & B_y &z|
\end{equation*}


It follows from \ref{eq:basis_vector_products} that for any two vectors $\vb{A}$ and $\vb{B}$ 
\begin{equation}
\vb{A} \cross \vb{B} = - \vb{B} \cross \vb{A}
\label{eq:cross_product_reflection}
\end{equation}

\subsection{Triple products}
Since the cross product of two vector is itself a vector, it can be dotted or crossed with a third vector to form a \textit{triple} product. 

\subsubsection{Scalar triple product}
The norm of $\vb{A} \vdot (\vb{B} \cross \vb{C})$ is the volume of a parallelepiped generated by $\vb{A}$, $\vb{B}$ and $\vb{C}$, since $\abs{ \vb{B} \cross \vb{C} }$ is the area of the base, while the dot product multiplies that number by the length of the projection of $\vb{A}$ orthogonal to the area. The choice of which vectors make the base of the parallelepiped is arbitrary, while the volume is independent of that choice, therefore: 

\begin{equation}
\vb{A} \vdot (\vb{B} \cross \vb{C}) = \vb{B} \vdot (\vb{C} \cross \vb{A}) = \vb{C} \vdot (\vb{A} \cross \vb{B})
\label{eq:scalar_triple_product}
\end{equation}

When $\vb{A}$, $\vb{B}$ and $\vb{C}$ form a \textit{right-handed} [\textit{left-handed}] basis --not necessarily orthonormal-- the above forms evaluate to the same \textit{positive} [\textit{negative}] value.  

Because of \ref{eq:cross_product_reflection} the following three extra forms are also equivalent among each other but they evaluate to the \textit{opposite} value, thus yielding a \textit{negative} [\textit{positive}] value when $\vb{A}$, $\vb{B}$ and $\vb{C}$ form a \textit{right-handed} [\textit{left-handed}] basis.

\begin{equation}
\vb{A} \vdot (\vb{C} \cross \vb{B}) = \vb{B} \vdot (\vb{A} \cross \vb{C}) = \vb{C} \vdot (\vb{B} \cross \vb{A})
\end{equation}

\subsubsection{Vector triple product}

The vector triple product can be simplified by the $\vb{BAC}\mathbf{-}\vb{CAB}$ rule:
\begin{equation}
\vb{A} \cross (\vb{B} \cross \vb{C}) = \vb{B} (\vb{A} \vdot \vb{C}) - \vb{C} (\vb{A} \vdot \vb{B})
\label{eq:vector_triple_product}
\end{equation}

Note that 

\begin{equation}
(\vb{A} \cross \vb{B}) \cross \vb{C} = - \vb{C} \cross (\vb{A} \cross \vb{B}) = \vb{B} (\vb{A} \vdot \vb{C}) - \vb{A} (\vb{B} \vdot \vb{C})
\label{eq:vector_triple_product_2}
\end{equation}

so changing the grouping yields a different vector, namely

\begin{equation*}
\vb{A} \cross (\vb{B} \cross \vb{C}) - (\vb{A} \cross \vb{B}) \cross \vb{C} = \vb{A} (\vb{B} \vdot \vb{C})  - \vb{C} (\vb{A} \vdot \vb{B})
\end{equation*}

All \textit{higher} products can be similarly reduced, so that it is never necessary for an expression to contain more than one cross product in any term. For instance
\begin{equation}
\begin{aligned} 
(\vb{A} \cross \vb{B}) \vdot (\vb{C} \cross \vb{D}) &= (\vb{A} \vdot \vb{C}) (\vb{B} \vdot \vb{D}) - (\vb{A} \vdot \vb{D}) (\vb{B} \vdot \vb{C})\, ;  \\
\vb{A} \cross [\vb{B} \cross (\vb{C} \cross \vb{D})] &= \vb{B} [\vb{A} \vdot (\vb{C} \cross \vb{D})] -  (\vb{A} \vdot \vb{B})(\vb{C} \cross \vb{D})
\end{aligned}
\end{equation}

\subsubsection{Problem 1}
Prove that 
\begin{equation*}
[\vb{A} \cross (\vb{B} \cross \vb{C})] + [\vb{B} \cross (\vb{C} \cross \vb{A})] +  [\vb{C} \cross (\vb{A} \cross \vb{B})] = 0  
\end{equation*}

By applying the BAC-CAB rule to each one of the above three terms one obtains the following six   terms\footnote{Each line is obtained by the previous one by putting in each place the vector which follows -- in the $ABC$ cycle-- the one in the same place in the line above.}
\begin{equation*}
\begin{aligned} 
\vb{A} (\vb{B} \vdot \vb{C}) &- \vb{C} (\vb{A} \vdot \vb{B})\, +\\
\vb{B} (\vb{C} \vdot \vb{A}) &- \vb{A} (\vb{B} \vdot \vb{C})\, +\\
\vb{C} (\vb{A} \vdot \vb{B}) &- \vb{B} (\vb{C} \vdot \vb{A})
\end{aligned}
\end{equation*}
Each scalar product appears twice, each time it is multiplied by the remaining vector but with opposite signs.\qed. 


\section{Position, Displacement, Separation Vectors}

The vector connecting the origin $\mathcal{O}$ to the location of a point in three dimensions is called the \textbf{position vector}
\begin{equation}
\vb{r} \equiv x \, \vu{i} + y \, \vu{j} + z \, \vu{k} 
\end{equation}

Following Griffiths, we reserve the letter $\vb{r}$ for that purpose. Its magnitude 

\begin{equation}
r = \sqrt{x^2 + y^2 + z^2} 
\end{equation}

is the distance from the origin, and 

\begin{equation}
\vu{r} = \frac{\vb{r}}{r} = \frac{x \, \vu{i} + y \, \vu{j} + z \, \vu{k}}{\sqrt{x^2 + y^2 + z^2}} 
\end{equation}

is a unit vector pointing radially outward. Following Griffiths, the \textbf{infinitesimal displacement vector} from $(x, y, z)$ to $(x+dx, y+dy, z+dz)$ is denoted as $d\vb{l}$

\begin{equation}
d\vb{l} = dx \, \vu{i} + dy \, \vu{j} + dz \, \vu{k} 
\end{equation}

Problems in electrodynamics frequently involve two points, typically a \textbf{source point}, $\vb{r}'$ and a \textbf{field point} $\vb{r}$ at which one needs to calculate the effect of the source. We use the symbol $\brcurs$ to denote the \textbf{separation vector} from the source point to the field point:

\begin{equation}
\begin{aligned}
\brcurs &\equiv \vb{r} - {\vb{r}}'\\
\rcurs &= \abs{\brcurs} = \abs{\vb{r} - {\vb{r}}'} \\
\hrcurs &= \frac{\brcurs}{\rcurs}
\end{aligned}
\label{eq:separation_vector}
\end{equation}


\section{How Vectors Transform}
\label{sec:how_vectors_transform}
Let two cartesian (orthonormal) reference frames in two dimensions, $\mathcal{F}$ and $\mathcal{F}'$, share a common origin $\mathcal{O}$. Let $\vu{i}, \vu{j}$ be unit vectors forming a basis of $\mathcal{F}$ and $\vu{i}', \vu{j}'$ some that form a basis of $\mathcal{F}'$. 

Let $\mathcal{F}'$ be rotated by the angle $\phi$ --around the origin-- with respect to $\mathcal{F}$, so that
\begin{equation}
\vu{i} \vdot \vu{i}' = \cos(\phi) = \vu{j} \vdot \vu{j}' 
\label{eq:cosphi_products}
\end{equation}
   
The components of a vector $\vb{A}$ with respect to the two frames are:

\begin{equation*}
\begin{aligned}
\vb{A} &= A_x \;\vu{i}\;  + A_y \; \vu{j}\\
\vb{A} &= A_x'\,\vu{i}'\, + A_y'\,\vu{j}'
\end{aligned}
\end{equation*}

If $\theta$ is the angle between $\vu{i}$ and $\vb{A}$, the angle between $\vu{j}$ and $\vb{A}$ is $\pi/2 - \theta$, therefore, 

\begin{equation*}
\begin{aligned}
A_x &= A \cos(\theta) \\
A_y &= A \sin(\theta) = A \cos(\pi/2 - \theta)
\end{aligned}
\end{equation*}

The angle that $\vb{A}$ forms with the axis $\vu{i}'$ of the primed frame is clearly $\theta - \phi$, therefore\footnote{Here we use the trigonometric identities $\sin(a \pm b) = \sin(a)\cos(b) \pm \cos(a)\sin(b)$ and  $\cos(a \pm b) = \cos(a)\cos(b) \mp \sin(a)\sin(b)$.}

\begin{equation*}
\begin{aligned}
A_x' &= A \cos(\theta') = A \cos(\theta - \phi) = A [\cos(\theta)\cos(\phi) + \sin(\theta)\sin(\phi)] \\
A_y' &= A \sin(\theta')\, = A \sin(\theta - \phi)\, = A [\sin(\theta)\cos(\phi) - \cos(\theta)\sin(\phi)] 
\end{aligned}
\end{equation*}
 
Substituting $A_x = A \cos(\theta)$ and $A_y = A \sin(\theta)$ in the above equations we obtain the law for transforming components to the rotated frame, namely

\begin{equation*}
\begin{aligned}
A_x' &= A_x \cos(\phi) + A_y \sin(\phi) \\
A_y' &= A_y \cos(\phi) - A_x \sin(\phi) 
\end{aligned}
\end{equation*}

In matrix notation:

\begin{equation}
\mqty[ A_x' \\
A_y'] = 
\mqty|  \cos(\phi) &  \sin(\phi) \\ 
- \sin(\phi) &  \cos(\phi) |
\mqty[ A_x \\
A_y]
\label{eq:2d_vector_transform}
\end{equation}

As expected, the matrix reduces to the identity in the limit of small $\phi$. The inverse transform is clearly obtained by changing $\phi$ into $-\phi$, thus inverting the sign of the off-diagonal terms in the above matrix. 

There is another way to derive \ref{eq:2d_vector_transform} that can be generalized to any number of dimensions. It is often presented by geometric figures with rectangular triangles to prove how the unit basis vectors $\vu{i}$ and $\vu{j}$ decompose in the rotated basis $\vu{i}'$ and $\vu{j}'$. But there is hardly the need of a figure, as  equation \ref{eq:cosphi_products} already contains half of the story.  Indeed $\vu{i}$'s decomposition as well as $\vu{j}$'s must reduce to the identity for $\phi=0$ and the sum of the two coefficients squared must be $1$. With these prescriptions\footnote{The positive sign of both the $\cos(\phi)$ terms is dictated by the need to recover the identity transformation in the limit $\phi \rightarrow 0$ . The signs of the $\sin(\phi)$ terms are determined -- assuming $\phi$ to be \textit{counter-clockwise} -- by the relative position of $\vu{i}$ with respect to $\vu{i}'$ (negative $\vu{j}'$ component) and that of $\vu{j}$ with respect to $\vu{j}'$ (positive $\vu{i}'$ component).} 

\begin{equation}
\begin{aligned}
\vu{i} &= \cos(\phi) \vu{i}' - \sin(\phi) \vu{j}' \\
\vu{j} &= \sin(\phi) \vu{i}' + \cos(\phi) \vu{j}'
\end{aligned}
\label{eq:2d_basis_transform}
\end{equation}

and the transformation \ref{eq:2d_vector_transform} is now re-obtained from \ref{eq:2d_basis_transform}

\begin{equation*}
\begin{aligned}
\vb{A}  &= A_x \vu{i} + A_y \vu{j}\\
		&= A_x (\cos(\phi) \vu{i}' - \sin(\phi) \vu{j}') \\
		&+ A_y (\sin(\phi) \vu{i}' + \cos(\phi) \vu{j}') \\
		&= (A_x \cos(\phi) + A_y \sin(\phi)) \vu{i}' \\
		&+ (A_y \cos(\phi) - A_x \sin(\phi)) \vu{j}'
\end{aligned}
\end{equation*}
 
Generalization starts by casting \ref{eq:2d_basis_transform} into a new form, that can be readily extended to three or more dimensions. For instance, in three dimensions: 

\begin{equation*}
\begin{aligned}
\vu{i} &= \;(\vu{i} \vdot \vu{i}')\, \vu{i}' + \;(\vu{i} \vdot \vu{j}')\, \vu{j}' + \;(\vu{i} \vdot \vu{k}')\, \vu{k}'\\
\vu{j} &= \;(\vu{j} \vdot \vu{i}')\, \vu{i}' + \;(\vu{j} \vdot \vu{j}')\, \vu{j}' + \;(\vu{j} \vdot \vu{k}')\, \vu{k}'\\
\vu{k} &=   (\vu{k} \vdot \vu{i}')\, \vu{i}' +   (\vu{k} \vdot \vu{j}')\, \vu{j}' +   (\vu{k} \vdot \vu{k}')\, \vu{k}'
\end{aligned}
\end{equation*}

and finally, in a more general, dimension-independent matrix notation: 

\begin{equation}
\mqty[\vu{e_1} \\ \vu{e_2} \\ \vu{e_3}] = \mqty|  
\vu{e_1} \vdot \vu{e_1}' & \vu{e_1} \vdot \vu{e_2}' & \vu{e_1} \vdot \vu{e_3}' \\
\vu{e_2} \vdot \vu{e_1}' & \vu{e_2} \vdot \vu{e_2}' & \vu{e_2} \vdot \vu{e_3}' \\
\vu{e_3} \vdot \vu{e_1}' & \vu{e_3} \vdot \vu{e_2}' & \vu{e_3} \vdot \vu{e_3}' |
\mqty[\vu{e_1}' \\ \vu{e_2}' \\ \vu{e_3}']
= S \mqty[\vu{e_1}' \\ \vu{e_2}' \\ \vu{e_3}']
\label{eq:3d_basis_transform}
\end{equation}

The matrix $S$, whose $S_{i,j}$ component equals the scalar product $\vu{e_i} \vdot \vu{e_j}'$ transforms the primed (rotated) unit basis vectors into the unprimed ones, therefore it is the three-dimensional analogue of \ref{eq:2d_basis_transform}. A comparison of the latter with \ref{eq:2d_vector_transform}  shows that the matrix $S$ transforming the \textit{components} of a vector from the unprimed to the primed basis, is actually identical to the \textit{transpose} of the matrix transforming the \textit{vectors} of the primed basis into the \textit{vectors} of the unprimed one. 

\begin{equation}
\mqty[A_1' \\ A_2' \\ A_3'] = S^\top \mqty[A_1 \\ A_2 \\ A_3] = \mqty|
\vu{e_1} \vdot \vu{e_1}' & \vu{e_2} \vdot \vu{e_1}' & \vu{e_3} \vdot \vu{e_1}' \\
\vu{e_1} \vdot \vu{e_2}' & \vu{e_2} \vdot \vu{e_2}' & \vu{e_3} \vdot \vu{e_2}' \\
\vu{e_1} \vdot \vu{e_3}' & \vu{e_2} \vdot \vu{e_3}' & \vu{e_3} \vdot \vu{e_3}  |
\mqty[\vu{e_1}' \\ \vu{e_2}' \\ \vu{e_3}']
\label{eq:3d_components_transform}
\end{equation}

Matrices like $S$ and $S^\top$, respectively defined in \ref{eq:3d_basis_transform} and \ref{eq:3d_components_transform}, are said to be \textit{orthogonal} due to their special properties. 
Indeed each matrix \textit{row} and each matrix \textit{column} represents the components of a unit vector of one orthonormal basis with respect to another orthonormal basis, so the norm of each row and of each column must be $1$. In $n$ dimensions this amounts to $2n$ constraints, so the total number of independent parameters is $n^2-2n$. Moreover, $S$ and $S^\top$ are each other's \textit{inverse}:
\begin{equation}
S S^\top = \mqty|
\vu{e_1} \vdot \vu{e_1} & \vu{e_2} \vdot \vu{e_1}  & \vu{e_3} \vdot \vu{e_1} \\  
\vu{e_1} \vdot \vu{e_2} & \vu{e_2} \vdot \vu{e_2}  & \vu{e_3} \vdot \vu{e_2} \\  
\vu{e_1} \vdot \vu{e_3} & \vu{e_2} \vdot \vu{e_3}  & \vu{e_3} \vdot \vu{e_3} |  = S^\top S
= \mqty|
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 | = I 
\label{eq:3d_ortho_matrices}
\end{equation}

In summary, the \textit{matrices} respectively transforming the \textit{basis vectors} (\ref{eq:3d_basis_transform}) and vector \textit{components} (\ref{eq:3d_components_transform}) are simply related by \textit{transposition}. This is a direct consequence --and one of the advantages-- of working with \textit{orthonormal bases}. For transformations connecting two \textit{general bases} --where one is (or both are) not orthonormal-- the analogue of matrix $S$ --let call it $R$ --  has the same structure, although the matrix element $R_{lm} = \vb{e}_l \vdot \vb{e}_m$ is the dot product of basis vectors of arbitrary norm, whence $R$ is generally not an orthogonal matrix. In such general context, the analogue of $S^\top$ is clearly\footnote{This fact agrees with the intuitive notion that \quotes{a rotation of the basis causes an opposite rotation of vector components}.} $R^{-1}$, the \textit{inverse} of $R$. 

\subsection{Euler Angles}
We saw that \textit{one} parameter -- the $\phi$ angle -- is needed to define the transformation between two orthonormal reference systems with a common origin $\mathcal{O}$ in 2D space. In 3D space the analogous transformation generally depends on \textit{three} parameters, as it will be evident from the construction of one such transformation and from an already cited general property of orthogonal matrices. 
In the following we use lower and capital letters to distinguish axes of the two systems, and we assume the latter to have the same \textit{orientation}, namely 

\begin{equation*}
(\vu{i} \cross \vu{j}) \vdot \vu{k} = (\vu{I} \cross \vu{J}) \vdot \vu{K} = 1 
\end{equation*}

Even without a figure we can agree that the two planes respectively orthogonal to $\vu{k}$ and $\vu{K}$ are either the same plane or they intersect at a line passing through the origin, the so-called \textit{line of nodes}. We do not discuss the first case ($\vu{k} = \pm \vu{K}$) as we already know how to handle 2D transformations, so let $\theta = \acos(\vu{k} \vdot \vu{K})$ be the angle (not null) formed by the two axes. The full transformation is thus defined as the \textit{product} of three 2D rotations, each one leaving one axis unchanged, to be performed in the following order:
\begin{enumerate}
\item Around $\vu{k}$ by the angle $\phi$ formed by the axis $\vu{i}$ with the line of nodes.  
\item Around the line of nodes by the angle $\theta$ so that $\vu{k}$ goes onto $\vu{K}$.
\item Around $\vu{K}$ by the angle $\psi$ so that the line of nodes goes onto $\vu{I}$.
\end{enumerate}

$\phi$, $\theta$ and $\psi$ are called the \textit{Euler angles}. 

Let denote as $R_\phi$, $R_\theta$ and $R_\psi$ the matrices associated to the transformation from unprimed to primed coordinates under the above rotations. Each matrix has the form given in \ref{eq:2d_vector_transform} for the transformation of coordinates under a 2D rotation. Adaptation to the 3D context amonts to ensure that points located on the rotation axis be unaffected. With these prescriptions
the transformation matrix is the product $R(\phi, \theta, \psi) = R_\psi R_\theta R_\phi$ where

\begin{equation}
R_\phi = \mqty| 
\cos(\phi) &  \sin(\phi) & 0 \\
-\sin(\phi) & \cos(\phi) & 0 \\
0 & 0 & 1 | 
\end{equation}

\begin{equation}
R_\theta = \mqty|
1 & 0 & 0 \\ 
0 & \cos(\theta) &  \sin(\theta) \\
0 &-\sin(\theta) & \cos(\theta) | 
\end{equation}
  
\begin{equation}
R_\psi = \mqty| 
\cos(\psi) &  \sin(\psi) & 0 \\
-\sin(\psi) & \cos(\psi) & 0 \\
0 & 0 & 1 | 
\end{equation}

Therefore 

\begin{equation}
\begin{aligned}
& R(\phi, \theta, \psi) = R_\psi \mqty| 
\cos(\phi) & \sin(\phi) & 0 \\
-\sin(\phi)\cos(\theta) & \cos(\phi)\cos(\theta) & \sin(\theta)\\
\sin(\phi)\sin(\theta) & -\cos(\phi)\sin(\theta) & \cos(\theta) | = \\
& \mqty|
\cos{\phi}\cos{\theta} - \sin{\phi}\sin{\theta}\cos{\psi} & \cos{\phi}\sin{\theta} - \sin{\phi}\cos{\theta}\cos{\psi} & \sin{\phi} \sin{\psi}\\
\cos{\phi}\sin{\theta}\cos{\psi} - \sin{\phi}\cos{\theta} & \cos{\phi}\cos{\theta}\cos{\psi} -\sin{\phi}\sin{\theta} & \cos{\phi} \sin{\psi}\\
\sin{\theta} \sin{\psi} & -\cos{\theta}\sin{\psi} & \cos{\psi} |
\end{aligned}
\end{equation}

The determinant $\det(R(\phi, \theta, \psi))$ is equal to $1$.  
As a reminder, if $R$ is assumed to be the matrix transforming the \textit{basis} vectors, then $R^\top$ is the matrix transforming vector \textit{components} across the two bases (and viceversa).  

\section{Differential Calculus}
\subsection{Gradient}
Given $T(x, y, z)$ a scalar function of several variables the \textit{linear} part of the function increment at a given point $(x, y, z)$ is given by
\begin{equation}
\begin{aligned}
d T &= \pdv{T}{x} dx + \pdv{T}{y} dy + \pdv{T}{z} dz \\
    &= \left( \pdv{T}{x} \vu{i} + \pdv{T}{y} \vu{j} + \pdv{T}{z} \vu{k}  \right) \vdot (dx \vu{i} + dy \vu{j} + dz \vu{k}) \\
    &= \grad{T} \vdot d\vb{l}
\end{aligned}
\label{eq:gradient}
\end{equation}

where 

\begin{equation}
\grad{T} = \pdv{T}{x} \vu{i} + \pdv{T}{y} \vu{j} + \pdv{T}{z} \vu{k} 
\end{equation}

is the \textit{gradient} of $T$. 

The geometrical meaning of the gradient becomes clear by rewriting the dot product in \ref{eq:gradient} as

\begin{equation}
dT = \grad{T} \vdot d\vb{l} = \abs{\grad{T}} \abs{d\vb{l}} \cos \theta
\end{equation}

where $\theta$ is the angle between $\grad{T}$ and $d\vb{l}$. With fixed $\abs{d\vb{l}}$ the maximum of $dT$ then occurs when $\theta = 0$, therefore:

\textit{The gradient $\grad{T}$ points in the direction of maximum increase of the function $T$, while $\abs{\grad{T}}$ is the rate of increase along this maximal direction.}

The condition $\grad{T} = \vb{0}$ defines a \textit{stationary point}. It could be a maximum (a summit), a minimum (a valley), a saddle point (a pass) or a \quotes{shoulder}. 
 
\subsubsection*{Example}
Let $r \equiv \abs{\vb{r}} = \sqrt{x^2 + y^2 + z^2}$, be the \textit{magnitude} of the \textit{position} vector. Compute the gradient of different powers of $r$. 

\begin{align*} 
\grad{r} &= \frac{\vb{r}}{r} = \hat{\vb{r}} \\
\grad{r^2} &= 2 r \grad{r} = 2r \vu{r} = 2 \vb{r} \\
\grad{\left( \frac{1}{r} \right)} &= - \frac{1}{r^2} \grad{r} = - \frac{\hat{\vb{r}}}{r^2} = - \frac{\vb{r}}{r^3} 
\end{align*}

\begin{equation} 
\grad(r^n) = n r^{n-1} \grad{r} =  n r^{n-1} \vu{r} =  n r^{n-2} \vb{r}
\label{eq:grad_r_powers}
\end{equation}

\subsubsection*{Problem 13}
Let $s \equiv \abs{\vb{r} - \vb{r}'}$, be the \textit{magnitude} of the \textit{separation} vector from a \textit{fixed} point $\vb{r}'$ to the point $\vb{r}$. Show that
\begin{enumerate}[a)]
\item $\grad(s^2) = 2 \vb{s}$ 
\item $\grad(1/s) = - \vu{s}/s^2$ 
\item What is the general formula for $\grad(s^n)$?
\end{enumerate}

We note that $\vb{r}'$ is a constant term in the evaluation of derivatives, therefore
the gradient of powers of $s$ have the form \ref{eq:grad_r_powers}, therefore  

\begin{enumerate}[a)]
\item
\begin{equation*}
\grad(s^2) = 2s \vu{s} = 2 \vb{s}
\end{equation*}
\item
\begin{equation*}
\grad(1/s) = - \frac{\vu{s}}{s^2}
\end{equation*}
\item
\begin{equation}
\grad(s^n) = n s^{n-1} \grad{s} = n s^{n-1} \vu{s} 
\end{equation}
\end{enumerate}

\subsubsection*{Problem 14}
Suppose that $f$ is a function of two variables only. Show that $\grad f$ transforms as a vector under rotations. \\

The transformation of the basis vectors as in \ref{eq:2d_basis_transform} implies that the \textit{primed} basis is obtained by a rotation about the origin of the \textit{unprimed} basis by an angle $\phi$. 

It turns out (see equation \ref{eq:2d_vector_transform}) that the corresponding transformation of vector \textit{components} from the original basis to the rotated one is carried out by the following matrix  

\begin{equation}
A(\phi) = \mqty| \cos(\phi) & \sin(\phi) \\
-\sin(\phi) & \cos(\phi)|
\label{eq:rotphi} 
\end{equation}  

whereby a vector $\vb{r}$ takes components $(x', y')$ in the rotated basis and these can be obtained from the unprimed components $(x, y)$ that $\vb{r}$ takes in the original basis by multiplication with the matrix $A(\phi)$: 

\begin{equation*}
\mqty[ x' \\ y'] = A(\phi) \mqty[ x \\ y] 
\end{equation*}  

In the following we need to transform from the primed to the original, which can be done with the inverse of $A(\phi)$, that happens to be just $A(-\phi)$

\begin{equation*}
\mqty[ x \\ y] = A(-\phi) \mqty[ x' \\ y'] 
\end{equation*}  

Indeed, in order to obtain the gradient in the primed system, we use the following derivatives:

\begin{align*} 
\pdv{f}{x'} &= \pdv{f}{x} \pdv{x}{x'}    + \pdv{f}{y} \pdv{y}{x'}    \\
		    &= \pdv{f}{x} A(-\phi)_{1,1} + \pdv{f}{y} A(-\phi)_{2,1} \\
\pdv{f}{y'} &= \pdv{f}{x} \pdv{x}{y'}    + \pdv{f}{y} \pdv{y}{y'}    \\
		    &= \pdv{f}{x} A(-\phi)_{1,2} + \pdv{f}{y} A(-\phi)_{2,2}
\end{align*}

which nicely reduce to 

\begin{equation}
\grad'\, f = (A(-\phi))^\top \grad\, f = A(\phi) \grad\, f
\label{eq:grad_transform}
\end{equation}  
\qed

\subsection{Divergence}
For a given vector field $\vb{v}(\vb{r})$ the following notation 

\begin{equation}
\begin{aligned}
\div{\vb{v}} &= \left( \vu{i} \pdv{}{x} + \vu{j} \pdv{}{y} + \vu{k} \pdv{}{z} \right) 
                      \vdot \left( v_x\, \vu{i} + v_y\, \vu{j} + v_z\, \vu{k} \right) \\
             &= \pdv{v_x}{x} + \pdv{v_y}{y} + \pdv{v_z}{z}				
\end{aligned}
\label{eq:divergence}
\end{equation}

is used to denote the \textit{divergence} of the vector field.

\subsubsection*{Problem 16}
Compute the divergence of the vector function
\begin{equation*}
\vb{v} \frac{\vu{r}}{r^2}
\end{equation*}  
The answer may surprise you... can you explain that?

First we reproduce here a key finding about \textit{radial fields} also exposed in chapter \ref{ch:Felsager_01}. Let $\vb{v}$ be a vector field depending on the magnitude $r$ of the position vector $\vb{r} = r \vu{r}$.

\begin{equation*}
\vb{v}(\vb{r}) = f(r) \hat{\vb{r}} 
\end{equation*}

where $f$ is any derivable scalar function. It turns out that

\begin{equation}
\begin{aligned} 
\div{\vb{A}} \equiv \div{\left[ f(r) \, \vu{r} \right]} 
&= \frac{2 \, f}{r} + f' = \frac{1}{r^2} \pdv{\left( r^2 f \right) }{r}
\end{aligned}
\end{equation}

where the last expression coincides with the known formula for the contribution of the radial term to the divergence when the field components are given in \textit{spherical coordinates}. For a radial field whose intensity is proportional to $1/r^2$ this expression is identically zero everywhere except at the origin $r=0$ where the expression becomes undefined. 

This is consistent with the fact that the \textit{inverse square law} causes the \textit{flux} of the field $\vb{v}$ over the surface of any volume to vanish iff the latter does not include the origin. On the contrary, the flux over the surface of any volume enclosing the origin has the constant value $4 \pi$, suggesting to define the divergence as $4 \pi \delta^3(\vb{r})$ where $\delta^3$ is the Dirac distribution.   

\subsubsection*{Problem 17}
In two dimensions, show that the divergence transforms as a \textit{scalar} under rotations. \\

Using \ref{eq:2d_vector_transform}

\begin{equation*}
\mqty[ \vb{v}_{x'} \\ \vb{v}_{y'}] = A(\phi) \mqty[ \vb{v}_{x} \\ \vb{v}_{y}] 
\end{equation*}  

where $A(\phi)$ is given in \ref{eq:rotphi}. Therefore

\begin{align*} 
\pdv{\vb{v}_{x'}}{x'} &= \pdv{\vb{v}_{x'}}{x} \pdv{x}{x'} + \pdv{\vb{v}_{x'}}{y} \pdv{y}{x'}    \\
&= \cos \phi \pdv{(\vb{v}_{x} \cos \phi + \vb{v}_{y} \sin \phi)}{x} + \sin \phi \pdv{(\vb{v}_{x} \cos \phi + \vb{v}_{y} \sin \phi)}{y} \\
\pdv{\vb{v}_{y'}}{y'} &= \pdv{\vb{v}_{y'}}{x} \pdv{x}{y'} + \pdv{\vb{v}_{y'}}{y} \pdv{y}{y'}	\\
&= -\sin \phi \pdv{(\vb{v}_{y} \cos \phi - \vb{v}_{x} \sin \phi)}{x} + \cos \phi \pdv{(\vb{v}_{y} \cos \phi - \vb{v}_{x} \sin \phi)}{y}
\end{align*}

Rearranging terms in the above expressions

\begin{align*} 
\pdv{\vb{v}_{x'}}{x'} &= \pdv{\vb{v}_x}{x} C^2 + \pdv{\vb{v}_y}{y} S^2 + \pdv{\vb{v}_y}{x} S C + \pdv{\vb{v}_x}{y} S C \\
\pdv{\vb{v}_{y'}}{y'} &= \pdv{\vb{v}_x}{x} S^2 + \pdv{\vb{v}_y}{y} C^2 - \pdv{\vb{v}_y}{x} S C - \pdv{\vb{v}_x}{y} S C \\
\end{align*}

(where $C = \cos(\phi)$, $S = \sin(\phi)$) one finally obtains 

\begin{equation}
\pdv{\vb{v}_{x'}}{x'} +  \pdv{\vb{v}_{y'}}{y'} = \pdv{\vb{v}_{x}}{x} + \pdv{\vb{v}_{y}}{y}
\end{equation}
\qed


\subsection{Curl}

The curl of a vector field $\vb{v}(\vb{r})$ is defined as follows

\begin{equation}
\begin{aligned}
\grad \cross \vb{v} &= \mqty|
\vu{i} & \vu{j} & \vu{k} \\
\partial_x & \partial_y & \partial_z \\
\vb{v}_x & \vb{v}_y & \vb{v}_z &  | \\
&= \vu{i} \left( \pdv{\vb{v}_z}{y} - \pdv{\vb{v}_y}{z}  \right)
 + \vu{j} \left( \pdv{\vb{v}_x}{z} - \pdv{\vb{v}_z}{x}  \right) 
 + \vu{k} \left( \pdv{\vb{v}_y}{x} - \pdv{\vb{v}_x}{y}  \right) 
\end{aligned}
\end{equation}  

\subsubsection*{Example}
Let $\vb{V}$ be a vector field \quotes{rotating} around the $\vu{k}$ axis of a cartesian frame, with a magnitude equal to $r^n$, where $r$ is the distance from the axis $\vu{k}$, namely

\begin{equation}
\vb{V} = r^n (\vu{k} \cross \vu{r})
\label{eq:curl_example_field}
\end{equation}  

We compute the curl of $\vb{V}$ by making use of the general identity

\begin{equation}
\curl{(f \vb{A})} = f (\curl{\vb{A}}) - \vb{A} \cross (\grad{f})
\label{eq:product_curl}
\end{equation} 

whereby 

\begin{equation}
\curl{\vb{V}} = r^n (\curl{(\vu{k} \cross \vu{r})}) - (\vu{k} \cross \vu{r}) \cross (\grad{r^n})
\label{eq:rotV}
\end{equation}

Making components explicit, the unit vectors defining $\vb{V}$ are     
\begin{align*}
\vu{k} &= \;0\, \vu{i}\; + \;0\, \vu{j}\; + \;1\, \vu{k}		\\ 
\vu{r} &= \frac{x }{\sqrt{x^2 + y^2}}\, \vu{i} + \frac{y}{\sqrt{x^2 + y^2}}\, \vu{j} \\
\vu{k} \cross \vu{r} &= -\frac{y}{\sqrt{x^2 + y^2}}\, \vu{i} + \frac{x}{\sqrt{x^2 + y^2}}\, \vu{j}
\end{align*} 

whence 

\begin{equation}
\begin{aligned}
\curl{(\vu{k} \cross \vu{r})} &= \vu{k} \left[ \partial_x \left( \frac{x}{\sqrt{x^2 + y^2}} \right) 
                                            +  \partial_y \left( \frac{y}{\sqrt{x^2 + y^2}} \right) \right] \\
                              &= \vu{k} \left[ \frac{2}{\sqrt{x^2 + y^2}} - \frac{x^2 + y^2}{(x^2 + y^2)^{3/2}} \right] \\
                              &= \frac{\vu{k}}{r}
\end{aligned} 
\end{equation}

Using formula \ref{eq:vector_triple_product_2} for the triple vector product\footnote{In this case it is sufficient to consider the result of combining mutually orthogonal unit vectors with the cross product.} 

\begin{equation*}
(\vu{k} \cross \vu{r}) \cross \vu{r} = -\vu{k} 
\end{equation*}

Now recalling \ref{eq:grad_r_powers}  

\begin{equation*}
\begin{aligned}
\grad (r^n) &= n r^{n-1} \vu{r} \\
(\vu{k} \cross \vu{r}) \cross \grad (r^n) &= n r^{n-1} (\vu{k} \cross \vu{r}) \cross \vu{r} \\
                                          &= - n r^{n-1} \vu{k} 
\end{aligned} 
\end{equation*}

Putting it all together 

\begin{equation}
\begin{aligned}
\curl{\vb{V}} &= r^n \curl{(\vu{k} \cross \vu{r})} - (\vu{k} \cross \vu{r}) \cross (\grad{r^n}) \\
              &= r^n \frac{\vu{k}}{r} + n r^{n-1} \vu{k}\\
              &= (n+1) r^{n-1} \vu{k} 
\end{aligned}
\label{eq:curl_example_value}
\end{equation}

We see from \ref{eq:curl_example_value} that a field $\vb{V}$ given by \ref{eq:curl_example_field} produces a curl directed along the axis $\vu{k}$ whose intensity is proportional to $r^{n-1}$. 

To check the correctness of this result we examine the \textit{line integral}

\begin{equation*}
\oint_{\Gamma} \vb{V} \vdot d\vb{l} = \oint_{\Gamma} r^n (\vu{k} \cross \vu{r}) \vdot d\vb{l}  
\end{equation*}
  
This integral is easy to evaluate when $\Gamma$ is a \textit{circle} of radius $R$ lying in a plane orthogonal to $\vu{k}$ and centered in the origin, because in that case 
\begin{itemize}
\item $\vu{k} \cross \vu{r}$ is a unit vector \textit{parallel} to $d\vb{l}$, and
\item $r$ takes the fixed value $R$ and that can be moved outside of the integral.
\end{itemize}

therefore 
\begin{equation}
\begin{aligned}
\oint_{\Gamma} \vb{V} \vdot d\vb{l} &= R^n \oint_{\Gamma} dl \\
                                    &= R^n \int_0^{2\pi} R d\phi 
                                    &= 2\pi R^{n+1} 
\end{aligned}
\label{eq:field_circolation}
\end{equation}
As expected\footnote{The curve total length is proportional to $R$, so integrating a function inversely proportional to $R$ yelds a result that does not depend on $R$}, the line integral evaluates to a constant ($2\pi$) for $n=-1$ when 

\begin{equation}
\vb{V} = \frac{\vu{k} \cross \vu{r}}{r} \\
\label{eq:constant_circolation_power}
\end{equation}

According to Stokes theorem, the same value of the line integral \ref{eq:field_circolation} must also be obtained when computing the flux of $\curl{\vb{V}}$ across any surface bounded by the circle $\Gamma$. Using \ref{eq:curl_example_value} we thus evaluate the \textit{surface integral}\footnote{The surface $\Sigma$ being the \textit{disc} bounded by the circle $\Gamma$ in the integral of \ref{eq:field_circolation}.}

\begin{equation}
\begin{aligned}
\int_{\Sigma} (\curl{\vb{V}}) \vdot d\vb{a} 
	&= 2\pi (n+1) \int_{\Sigma} r^{n-1}\, \vu{k} \vdot d\vb{a} \\
	&= 2\pi (n+1) \int_{\Sigma} r^{n-1}\, d a \\
	&= 2\pi (n+1) \int_0^R r^{n-1} dr \int_0^{2 \pi}  (r d\phi) \\
	&= 2\pi (n+1) \int_0^R r^n dr
\end{aligned}
\label{eq:rotation_flux_integral}
\end{equation}

Depending on $n$, the \textit{primitive} of the last integral in the above equation evaluates to different analytic expressions (see Dwight 81.9 and 82.1)

\begin{equation}
\begin{aligned}
\int r^n dr &=  \frac{R^{n+1}}{n+1}\:\:\:\:\:\: [n \neq -1] \\
	        &=  \log(\abs{r})\:\:\:\:           [n = -1]       
\end{aligned}
\label{eq:rotation_flux_primitives}
\end{equation}

We see in \ref{eq:rotation_flux_primitives} that the $\log$  primitive, the one applicable for $n = -1$, is  undefined at $r=0$ while the other is undefined at $r=0$ when $n < -1$, therefore:\\

\textit{No primitive is defined at $r=0$ for $n < 0$}. \\ 

A definite expression for the \textit{flux} is available for $n \geq 0$, namely 
\begin{equation}
n \geq 0 \: \longrightarrow \: \int_{\Sigma} (\curl{\vb{V}}) \vdot d\vb{a} = 2\pi R^{n+1} 
\label{eq:flux}
\end{equation}

and it is identical to \ref{eq:field_circolation}, the one previously obtained from \textit{circulation}. 

\subsubsection*{***}
Let focus on the vector field of \ref{eq:curl_example_field} with $n = -1$. In this case equation \ref{eq:curl_example_value} yields a $0/0$ undetermined form for $\curl{\vb{V}}$ at $r=0$. If we exclude the origin, thereby restricting attention on the region $r > 0$, equation \ref{eq:curl_example_value} says that $\curl{\vb{V}}$ is identically zero in that region. 

All this is consistent with findings related to any annular region delimited by two concentring circles centered on the axis $\vu{k}$. In that region any circle yields the same circulation ($2\pi$), thereby implying\footnote{Making a radial cut that connects the two circles, a circuit embracing the whole annular region travels the two circles in opposite directions.} that the flux of $\curl{\vb{V}}$ over the region is zero.

When $n = -1$ replacing \ref{eq:curl_example_value} with the following expression for $\curl{\vb{V}}$ 
\begin{equation}
\curl{\vb{V}} = \curl{ \left( \frac{\vu{k} \cross \vu{r}}{r} \right) } = 2\pi \delta(r) \vu{k}
\label{eq:flux}
\end{equation}

makes the computed flux agree with circulation! 

\subsubsection*{Problem 20}
Construct a non trivial vector field with zero divergence and zero curl everywhere.

$\curl{(\grad{\phi})} = \vb{0}$ suggests to take $\vb{V} = \grad{\phi}$. The requirement of zero divergence then implies 
\begin{equation*}
\begin{aligned}
\div{\vb{V}} &= \div{(\grad{\phi})} \\
             &= \laplacian{\phi} = 0 
\end{aligned}
\end{equation*}

Therefore, any scalar \textit{field} $\phi$, solution of the \textit{homogeneous} Laplace equation would do. 

\subsection{Product Rules}

\textbf{Ordinary} derivatives are \textit{linear} in their argument, namely\footnote{In this section $k$ is a \textit{scalar}, while $f$ and $g$ are \textit{scalar functions}.}

\begin{equation*}
\begin{aligned}
\dv{x} (f + g) &= \dv{f}{x} + \dv{g}{x} \\
\dv{x}(kf)     &= k \dv{f}{x}
\end{aligned}
\end{equation*}

\textbf{Vector} derivatives operate on \textit{fields}\footnote{\textit{Scalar field} means \quotes{\textit{function} whose domain is the set of points of a certain region of three-dimensional space}. \textit{Vector field}  means \quotes{$3$ \textit{functions} whose domain is the set of points of a certain region of three-dimensional space}.}

\begin{equation*}
\begin{aligned}
\grad (f + g) &= \grad{f} + \grad{g} \\
\div (\vb{A} + \vb{B}) &= \div \vb{A} + \div \vb{B} \\
\curl (\vb{A} + \vb{B}) &= \curl \vb{A} + \curl \vb{B} \\
\end{aligned}
\end{equation*}

\begin{equation*}
\begin{aligned}
\grad (kf) &= k \grad{f} \\
\div (k \vb{A}) &= k \div \vb{A} \\
\curl (k\vb{A}) &= k \curl \vb{A} \\
\end{aligned}
\end{equation*}

\textbf{Ordinary} derivatives of \textit{products} and \textit{quotients} obey the following rules 

\begin{equation*}
\begin{aligned}
\dv{x} (fg) &= f \dv{g}{x} + g \dv{f}{x} \\ \\
\dv{x}\left(\frac{f}{g}\right) &= \frac{1}{g^2}\left(g \dv{f}{x} - f \dv{g}{x}\right)
\end{aligned}
\end{equation*}

Among \textbf{vector} derivatives 
\begin{itemize}
\item Divergence ($\div{}$), and
\item Curl ($\curl{}$)
\end{itemize}
operate of \textit{vector} fields, while
\begin{itemize}
\item Gradient ($\grad{}$)
\end{itemize}
operates on \textit{scalar} fields.

Products that produce a \textit{scalar} field are 
\begin{itemize}
\item $fg \:\:\:$ (product of two scalar functions),
\item $A \vdot B$ (dot product of two vector fields).
\end{itemize}

while those producing a vector field are  
\begin{itemize}
\item $f \vb{A}\:\:\:$ (scalar times vector),
\item $A \cross B$     (cross product of two vector fields).
\end{itemize}

Accordingly, there are six \textit{product rules}, two for each operator:

\begin{subequations}
\label{eq:product_rules_gradient}
\begin{align}
\grad(fg) &= f \grad g + g \grad f \\
\grad(\vb{A} \vdot \vb{B}) &= \vb{A} \cross (\curl \vb{B}) + \vb{B} \cross (\curl \vb{A}) + (\vb{A} \vdot \grad) \vb{B} + (\vb{B} \vdot \grad) \vb{A} 
\end{align}
\end{subequations}

\begin{subequations}
\label{eq:product_rules_divergence}
\begin{align}
\div(f \vb{A}) &= f (\div \vb{A}) + \vb{A} \vdot (\grad f) \\
\div(\vb{A} \cross \vb{B}) &= \vb{B} \vdot (\curl{\vb{A}}) - \vb{A} \vdot (\curl{\vb{B}})
\end{align}
\end{subequations}

\begin{subequations}
\label{eq:product_rules_curl}
\begin{align}
\curl(f \vb{A}) &= f (\curl \vb{A}) - \vb{A} \cross (\grad f) \\
\curl(\vb{A} \cross \vb{B}) &= (\vb{B} \vdot \grad) \vb{A} - (\vb{A} \vdot \grad) \vb{B} + \vb{A} (\div{\vb{B}})  - \vb{B} (\div{\vb{A}})
\end{align}
\end{subequations}

There is also one \textit{quotient rule} for each operator:

\begin{subequations}
\label{eq:quotient_rules}
\begin{align}
\grad \left( \frac{f}{g}\right) &= \frac{g \grad f - f \grad g}{g^2} \\
\div \left( \frac{\vb{A}}{g}\right) &= \frac{g (\div \vb{A}) - \vb{A} \vdot (\grad g)}{g^2}\\
\curl \left( \frac{\vb{A}}{g}\right) &= \frac{g (\curl \vb{A}) - \vb{A} \cross (\grad g)}{g^2}
\end{align}
\end{subequations}

\subsubsection*{Problem 22}
What does $(\vb{A} \vdot \grad) \vb{B}$ mean?

Let $\vb{A} = A_x \vu{i} + A_y \vu{j} + A_z \vu{k}$, $\vb{B} = B_x \vu{i} + B_y \vu{j} + B_z \vu{k}$, so that 

\begin{align*}
(\vb{A} \vdot \grad) &= (\vu{i} A_x + \vu{j} A_y + \vu{k} A_z ) \vdot (\vu{i} \partial_x + \vu{j} \partial_y + \vu{k} \partial_z) \\ 
                     &= (A_x \partial_x + A_y \partial_y + A_z \partial_z)
\end{align*}
whence  
\begin{align*}
(\vb{A} \vdot \grad) \vb{B} &= \vu{i} (A_x \partial_x + A_y \partial_y + A_z \partial_z) B_x \\
							&+ \vu{j} (A_x \partial_x + A_y \partial_y + A_z \partial_z) B_y \\
							&+ \vu{k} (A_x \partial_x + A_y \partial_y + A_z \partial_z) B_z \\
							&= [\vb{A} \vdot \grad(B_x)]\, \vu{i}  + [\vb{A} \vdot \grad(B_y)]\, \vu{j} + [\vb{A} \vdot \grad(B_z)]\, \vu{k}
\end{align*}


\subsection{Second Derivatives}

There are \textit{five} types of \textit{second} derivatives that can be obtained by combining the \textit{gradient},  \textit{divergence}, and \textit{curl}:\\

\begin{enumerate}[(i)]
\item Divergence of gradient: $\div (\grad S) = (\partial_x^2 + \partial_y^2 + \partial_z^2) S \equiv \laplacian S$ 
\item Curl of gradient: $\curl (\grad S) = \vb{0}$ 
\item Gradient of divergence: $\grad (\div \vb{V})$
\item Divergence of curl: $\div (\curl \vb{V}) = 0$
\item Curl of curl: $\curl (\curl \vb{V}) = \grad (\div \vb{V}) - \laplacian(\vb{V})$
\end{enumerate}

The term $\laplacian (\vb{V})$ in the last formula is the \textit{Laplacian} of a vector\footnote{A vector each of whose components is the Laplacian of the corresponding component in the original vector.}. The following geometric expression derives directly from the same formula: 

\begin{equation*}
\laplacian \vb{V} \equiv \grad (\div \vb{V}) - \curl (\curl \vb{V})
\end{equation*}

In conclusion, the \textit{Laplacian} and the \textit{gradient-of-divergence} are the only two kinds of second derivatives.

\section{Integral Calculus}
\subsection{Line, Surface and Volume Integrals}

\begin{enumerate}[(a)]
\item \textbf{Line Integrals}\\
A line integral involves a vector field $\vb{v}$ and a portion of a prescribed \textit{path} (a curve) from a point $\vb{a}$ to a point $\vb{b}$. Each infinitesimal path element $\dd\vb{l}$ contributes to the integral with the scalar product $\vb{v} \vdot \dd\vb{l}$. This integral is denoted by the expression
\begin{equation*}
\int_{\vb{a}}^{\vb{b}} \vb{v} \vdot \dd\vb{l}
\end{equation*}

When the path is a closed curve $\Gamma$ and the integral is extended to include contributions from the whole of it, the integral is denoted with a different symbol, namely
\begin{equation*}
\oint_{\Gamma} \vb{v} \vdot \dd\vb{l}
\end{equation*}

In general, a line integral between two points depends on the path. A field $\vb{v}$ whose line integral is path independent, being only determined by the end points, is called a \textit{conservative field}\footnote{For any such field $\vb{v}$ a scalar field $\phi$ exists such that $\vb{v} = \grad \phi$.}.
 
\item \textbf{Surface Integrals}\\
A surface integral involves a vector field $\vb{v}$ and a prescribed \textit{surface} $\Sigma$. Each infinitesimal patch element $\dd\vb{a}$ is a vector whose direction is orthogonal to the surface and whose norm equals the element area. Each element contributes to the integral with the scalar product $\vb{v} \vdot \dd\vb{a}$. This integral is denoted by the expression
\begin{equation*}
\int_{\Sigma} \vb{v} \vdot \dd\vb{a}
\end{equation*}

If $\vb{v}$ represents the \textit{flow} of a fluid (mass per unit area per unit time) the integral represents the total \textit{flux} (mass per unit time), which explains the alternative name \textit{flux} given to the surface integral.  

Of course there are \textit{two} directions orthogonal to any surface, therefore one needs some convention to assign a definite sign to a surface integral. For a \textit{closed} surface any patch element is assumed to be \textit{outwardly oriented}. By this convention, $\vb{v}$ would positively contribute to the integral at surface elements where it is pointing outside the volume enclosed by the surface. When extended to a closed surface the integral is denoted with the symbol 
\begin{equation*}
\oint_{\Sigma} \vb{v} \vdot \dd\vb{a}
\end{equation*}

Its value represents the flux (total quantity per unit time) leaving the enclosed volume.

When the surface is not closed, we consider integrals covering the portion of a surface delimited by a \textit{closed curve} $\Gamma$. For a given closed curve $\Gamma$ there are of course infinite surfaces containing that curve, and the flux of a field $\vb{v}$ will generally be different over different surfaces. 

There are special fields $\vb{v}$ whose flux if fully determined by the curve $\Gamma$, whereby they keep the same value when evaluated over different surfaces bounded by that curve\footnote{For any such field $\vb{v}$ a vector field $\vb{A}$ exists such that $\vb{v} = \curl \vb{A}$.}. 

\item \textbf{Volume Integrals}
A volume integral involves a \textit{scalar} field $\rho$ and a region of space $\mathcal{V}$. Each infinitesimal volume element $\dd\tau$ contributes to the integral with with $\rho \dd\tau$. This integral is denoted by the expression
\begin{equation*}
\int_{\mathcal{V}} \rho \dd\tau
\end{equation*}

It is also possible to define the volume integral of a \textit{vector} field $\vb{v}$ 

\begin{align*}
\int_{\mathcal{V}} \vb{v} \dd\tau &\equiv  \int_{\mathcal{V}} (\vb{v}_x \vu{i} + \vb{v}_y \vu{j} + \vb{v}_z \vu{k}) \dd\tau \\
                                  &\equiv  \vu{i}\; \int_{\mathcal{V}} \vb{v}_x \dd\tau\:\; +
                                           \vu{j}\; \int_{\mathcal{V}} \vb{v}_y \dd\tau\:\; + 
                                           \vu{k}\; \int_{\mathcal{V}} \vb{v}_z \dd\tau                                 
\end{align*}

\end{enumerate}


\subsection{The Fundamental Theorem of Calculus}

Let $f(x)$ be a function of one variable. The fundamental theorem of calculus says: 

\begin{equation}
\int_{a}^{b} \dv{f}{x} \dd x = f(b) - f(a)
\label{eq:fundamental_theorem_of_calculus}
\end{equation}

The theorem is a true assertion which relates the integral of a \textit{derivative} over some \textit{region} to the values of the \textit{function} at the \textit{boundary} of that region (the end points, in this case). In vector calculus we have identified three kinds of derivatives: \textit{gradients}, \textit{divergences}, and \textit{curls} and it turns out that each of these has its own kind of \quotes{fundamental theorem}.

\subsection{The Fundamental Theorem for Gradients}

\begin{equation}
\int_{\vb{a}}^{\vb{b}} (\grad T) \vdot \dd\vb{l} = T(\vb{b}) - T(\vb{a})
\label{eq:fundamental_theorem_for_gradient}
\end{equation}

Note that the right-hand side makes no reference to the \textit{path} connecting the end points. 
This means \textit{gradients} have the special property that their line integrals are path independent:


\subsubsection*{Corollary 1}
$\int_{\vb{a}}^{\vb{b}} (\grad T) \vdot \dd\vb{l}$ is independent of the path taken from $\vb{a}$ to $\vb{b}$.

\subsubsection*{Corollary 2}
$\oint (\grad T) \vdot \dd\vb{l} = 0$ since the end points are identical, hence $T(\vb{b}) - T(\vb{a}) = 0$.

\subsection{The Fundamental Theorem for Divergences}
This theorem also goes by the name of \textbf{Gauss's theorem}. It relates the \textit{flux} of a vector field over a closed surface $\Sigma$ to the volume integral of its divergence over the enclosed region $\mathcal{V}$.

\begin{equation}
\int_{\mathcal{V}} (\div \vb{v}) \dd\tau = \oint_{\Sigma} \vb{v} \vdot \dd\vb{a}
\label{eq:fundamental_theorem_for_divergence}
\end{equation}

One geometrical interpretation of this theorem is that any vector field with a well-defined divergence describes the flow of a \textit{conserved quantity}, whereby the net balance of its \textit{flux} across a closed surface is equal to the volume integral of $\div \vb{v}$, where the latter defines, at any given point, the amount per unit volume of that quantity entering into the region, at that point, in a unit of time.   

\subsection{The Fundamental Theorem for Curls}
This theorem also goes by the name of \textbf{Stokes' theorem}. It relates the \textit{circulation} of a vector field over a closed curve $\Gamma$ to the surface integral of its curl over any surface bounded by $\Gamma$.

\begin{equation}
\int_{\Sigma} (\curl \vb{v}) \vdot \dd\vb{a} = \oint_{\Gamma} \vb{v} \vdot \dd\vb{l}
\label{eq:fundamental_theorem_for_curl}
\end{equation}

Note that the right-hand side makes no reference to the \textit{surface} bounded by the curve $\Gamma$. 
This means \textit{curls} have the special property that their flux only depends on the surface \textit{boundary}:

\subsubsection*{Corollary 1}
$\oint \vb{v} \vdot \dd\vb{l}$ depends only on the boundary curve, not on the particular surface.

\subsubsection*{Corollary 2}
$\oint (\curl \vb{v}) \vdot \dd\vb{a} = 0$ on any closed surface, since the boundary line shrinks to a point, hence the right-hand side must vanish.

\subsection{Integration by Parts}
The technique known as \textit{integration by parts} exploits the product rule for derivatives:

\begin{equation*}
\dv{x} (fg) = f \dv{g}{x} + g \dv{f}{x}
\end{equation*}
 
Integrating both sides and invoking the fundamental theorem of calculus (\ref{eq:fundamental_theorem_of_calculus}) one obtains

\begin{equation*}
\int_a^b \dv{x} (fg) \dd{x} = \eval{fg}_a^b = \int_a^b f \dv{g}{x} \dd{x} + \int_a^b g \dv{f}{x} \dd{x}
\end{equation*}

or

\begin{equation}
\int_a^b f \dv{g}{x} \dd{x} = \eval{fg}_a^b - \int_a^b g \dv{f}{x} \dd{x}
\label{eq:integration_by_parts}
\end{equation}

Formula \ref{eq:integration_by_parts} applies when under the integral is the product of one function ($f$) and the \textit{derivative} of another ($g$). It says that you can \textit{transfer the derivative from $g$ to $f$} at the cost of a minus sign and a boundary term.

\subsubsection{Example 12}

Evaluate the integral

\begin{align*}
I = \int_0^{\infty} x e^{-x} \dd{x}                         
\end{align*}

The exponential can be expressed as a derivative:
\begin{equation*}
e^{-x} = \dv{(-e^{-x})}{x}
\end{equation*}

therefore

\begin{align*}
I = \int_0^{\infty} x \dv{(-e^{-x})}{x} &=  \int_0^{\infty} e^{-x} \dd{x} - \eval{x e^{-x}}_0^{\infty}\\
                                        &=  \int_0^{\infty} e^{-x} \dd{x} \\
                                        &=  - \eval{e^{-x}}_0^{\infty} = 1                                                  
\end{align*}
\qed

The same approach can be used when the expression under the integral involves a differential operator (gradient, divergence or curl) and that expression is one of the two terms arising when the operator is applied to a product. 


For \textbf{gradients}, \ref{eq:fundamental_theorem_for_gradient} and the first of \ref{eq:product_rules_gradient} imply 

\begin{equation}
\begin{aligned}
\int_a^b (f \grad g) \vdot \dd{\vb{l}} &= \int_a^b (\grad (fg) - g \grad f) \vdot \dd{\vb{l}} \\   
                                       &= \eval{fg}_a^b - \int_a^b g \grad f \vdot \dd{\vb{l}}
\end{aligned}
\label{eq:grad_integration_by_parts}
\end{equation}


For \textbf{divergences}, \ref{eq:fundamental_theorem_for_divergence} and the first of \ref{eq:product_rules_divergence} imply 

\begin{align*}
\int_{\mathcal{V}} \div(f \vb{A}) \dd{\tau} &= \int_{\Sigma} f \vb{A} \vdot \dd{\vb{a}} \\
										    &= \int_{\mathcal{V}}  f (\div \vb{A}) \dd{\tau} + \int_{\mathcal{V}} \vb{A} \vdot (\grad f) \dd{\tau} 
\end{align*}

whence 

\begin{equation}
\int_{\mathcal{V}}  f (\div \vb{A}) \dd{\tau} = \int_{\Sigma} f \vb{A} \vdot \dd{\vb{a}} - \int_{\mathcal{V}} \vb{A} \vdot (\grad f) \dd{\tau} 
\label{eq:div_integration_by_parts}
\end{equation}


For \textbf{curls}, \ref{eq:fundamental_theorem_for_curl} and the first of \ref{eq:product_rules_curl} imply 

\begin{align*}
\int_{\Sigma} \curl(f \vb{A}) \dd{\vb{a}} &=  \oint_{\Gamma} f \vb{A} \vdot \dd{\vb{l}}  \\
										  &= \int_{\Sigma} f  (\curl \vb{A}) \dd{\vb{a}} - \int_{\Sigma} [\vb{A} \cross (\grad f)] \vdot \dd{\vb{a}} 
\end{align*}

whence 

\begin{equation}
\int_{\Sigma} f  (\curl \vb{A}) \dd{\vb{a}} = \oint_{\Gamma} f \vb{A} \vdot \dd{\vb{l}} + \int_{\Sigma} [\vb{A} \cross (\grad f)] \vdot \dd{\vb{a}}  
\label{eq:curl_integration_by_parts}
\end{equation}
  
\section{Curvilinear Coordinates}
A characteristic of the so called \textit{curvilinear} coordinates is that they are referred to \textit{different bases} of unit vectors $\vb{e}_1$, $\vb{e}_2$, $\vb{e}_3$ at different \textit{points} of the space region of interest. In spite of the apparent complication, this may be a convenient choice when dealing with fields that exhibit a certain symmetry. Apart from requirements of continuity and differentiability, the most general field $\vb{v}$ may take on arbitrarily different values at different points $\vb{r}_1 \neq \vb{r}_2$.  By contrast, we call \textit{spherically symmetric} a vector field whose representative vector at any point is only a function of its distance $r = \abs{\vb{r}}$ from the origin. Therefore, while in the general case $\vb{v}$ can be thought of as a vector function of vector argument ($\:\:\vb{v} = \vb{f}(\vb{r})\:\:$) a spherically symmetric field is actually a vector function of the scalar argument $r$   ($\:\:\vb{v} = \vb{f}(r)\:\:$). 

The most widely used curvilinear coordinates are the  \textit{spherical} and \textit{cylindrical}. Their basis (at any given point in space) is a triple of \textit{orthonormal} vectors. Here as in \cite{Griffiths_4th} we obtain formulas for differential operators by transformations from curvilinear to Cartesian coordinates. A more detailed geometric treatment of curvilinear coordinates is provided in \cite{BudakFomin_1973}.  

We recall what already exposed in section \ref{sec:how_vectors_transform} about the transformation of vectors with respect to different \textit{orthogonal} bases, namely an \textit{unprimed basis} $\vu{e}_1, \vu{e}_2, \vu{e}_3$ and a \textit{primed basis} ${\vu{e}_1}',{\vu{e}_2}', {\vu{e}_3}'$. There we found that given $S$ the matrix expressing each \textit{vector} of the \textit{unprimed basis} as a linear combination of vectors of the  \textit{primed basis}, then $S^{-1}=S^{\top}$ does express the \textit{components} of a vector in the primed basis as linear combinations of its components in the unprimed basis. Because of the difference in transformation behaviour between vector components and basis vectors, \textit{vectors} are said to be \textbf{contravariant}.

There are other geometrical objects -- of which the \textit{gradient} of a scalar field is the prototype -- whose components actually transform in a change of basis like the basis vectors, and not like the components of a vector. These objects are called \textit{forms} and for their transformation behaviour are said to be \textbf{covariant}.\footnote{From more general treatments it is known that for each vector space it is possible to define a \textit{dual space}: a vector space whose elements are \textit{forms} i.e. linear functions of one or more vectors. Like vectors, forms can be expressed as linear combinations with respect to a \textit{dual basis}. If the vector space admits a scalar product, it is possible to establish an \textit{isomorphism} between vectors and forms. In that case one may use the same symbol to denote both the vector and the form, although the components of each with respect to the corresponding basis are generally different. The universally adopted convention to avoid the ambiguity is to denote the \textit{contravariant} components with respect to a basis with an \textit{upper index} and the \textit{covariant} components with respect to the dual basis with a \textit{lower index}. 
When adopting an orthonormal basis the contravariant and covariant components of a vector/form have identical numerical values. This fact does not imply that the one between vectors and forms is a somewhat pedantic distinction: it is not, as their components transform in a radically different way in a change of basis, even when the two bases involved are both orthonormal!}

\subsection{Spherical Coordinates}
Spherical coordinates can be defined with respect to a Cartesian frame $\vu{i}$, $\vu{j}$, $\vu{k}$ by specifying, for each \textbf{point in space}
\begin{itemize}
\item the \textit{norm} $r = \sqrt{x^2 + y^2 + z^2}$ of the radius vector $\vb{r}$; 
\item the \textit{polar} angle $\theta$ formed by $\vb{r}$ with the $\vu{k}$ axis;
\item the \textit{azimuthal} angle $\phi$ between the $\vu{i}$ axis and the projection of $\vb{r}$ onto the plane spanned by $\vu{i}$ and $\vu{j}$.
\end{itemize}
$\theta$ takes values in the range $[0, pi]$, $\phi$ in the interval $[0, 2\pi]$.

The following set of equations provides the Cartesian coordinates $(x, y, z)$ of a point in space that may be alternatively specified by its spherical coordinates $(r, \theta, \phi)$ 

\begin{equation}
\label{eq:spherical_to_cartesian}
\begin{aligned}
x &= r \sin (\theta) \cos (\phi) \\
y &= r \sin (\theta) \sin (\phi) \\
z &= r \cos (\theta)
\end{aligned}
\end{equation}

The inverse transformation is 
\begin{equation}
\label{eq:cartesian_to_spherical}
\begin{aligned}
r      &= \sqrt{x^2 + y^2 + z^2} \\
\theta &= \arccos (\frac{z}{\sqrt{x^2 + y^2 + z^2}}) \\
\phi   &= \arctan (\frac{y}{x})
\end{aligned}
\end{equation}

Equations \ref{eq:spherical_to_cartesian} and \ref{eq:cartesian_to_spherical} is all we need to relate the coordinates of a \textit{point} in the two systems.

For \textit{vectors}, at each space point $\vb{r}_o$ identified by spherical coordinates $r_o, \theta_o, \phi_o$ we adopt a distinct, orthonormal frame (a basis) consisting of
\begin{itemize}
\item $\vu{r}$, parallel to the vector $\vb{r}_o$,
\item $\vu{\theta}$, tangent to the curve $\vb{r}(r_o, \theta, \phi_o)$, and 
\item $\vu{\phi}$, tangent to the curve $\vb{r}(r_o, \theta_o, \phi)$.
\end{itemize}

This is all well and natural, but how do we get an explicit expression of the basis vectors $\vu{r}\,, \vu{\theta}\,, \vu{\phi}$ associated to a space point $\vb{r}$?   

The best strategy is to start by expanding the Cartesian basis vectors $\vu{i}, \vu{j}, \vu{k}$ in terms of the $\vu{r}\,, \vu{\theta}\,, \vu{\phi}$, then to obtain the latter in terms of the former by the inverse transformation. The first step is helped by two preliminary considerations:    
\begin{itemize}
\item For $\theta=0$, the cartesian axes $\vu{k}$ and $\vu{i}$ coincide with the \textit{radial} spherical axis $\vu{r}$ and the \textit{polar} spherical axis $\vu{\theta}$, respectively, so that $\vu{\phi}$ must also coincide with $\vu{j}$. 
\item The ordered triple $\vu{k}, \vu{i}, \vu{j}$ is right-handed as it does the ordered triple $\vu{r}\,, \vu{\theta}\,, \vu{\phi}$, and the two are actually one and the same thing for $\theta=0$. 
\end{itemize}

Computing derivatives of the Cartesian coordinates $z, x, y$ (in that order) from $r, \theta, \phi$ we obtain:

\begin{equation}
\label{eq:not_orthogonal_matrix}
\mqty[dz \\ dx \\ dy] = \mqty|
\cos(\theta) & -r \sin(\theta) \\
\sin(\theta)\cos(\phi) & r \cos(\theta)\cos(\phi) & -r \sin(\theta)\sin(\phi) \\
\sin(\theta)\sin(\phi) & r \cos(\theta)\sin(\phi) & +r \sin(\theta)\cos(\phi)
| \:  \mqty[dr \\ d\theta \\ d\phi]
\end{equation}

The matrix in the above equation resembles very much the transformation between two orthonormal bases, except for the factors $r$ and $r\sin(\theta)$ multiplying all elements in the second and third column, respectively. 
If we transfer those factors to the right-hand side we obtain 

\begin{equation}
\label{eq:orthomatrix}
\mqty[dz \\ dx \\ dy] = \mqty|
\cos(\theta) & -\sin(\theta) \\
\sin(\theta)\cos(\phi) & \cos(\theta)\cos(\phi) & -\sin(\phi) \\
\sin(\theta)\sin(\phi) & \cos(\theta)\sin(\phi) & +\cos(\phi)
| \:  \mqty[dr \\ r d\theta \\ r \sin(\theta)d\phi]
\end{equation}

The transformation matrix appearing in \ref{eq:orthomatrix} is \textit{orthogonal} with unit determinant, therefore it is a good candidate to relate the two orthonormal bases $\vu{k}, \vu{i}, \vu{j}$ and $\vu{r}\,, \vu{\theta}\,, \vu{\phi}$, but of course its \textit{transpose} is a good candidate as well. In order to decide between the two, we must determine whether \ref{eq:orthomatrix} defines the \textit{covariant} transformation of coordinates between \textit{forms} or the \textit{contravariant} transformation of coordinates between \textit{vectors}. 

Common wisdom suggests that being arrived at \ref{eq:orthomatrix} by taking derivatives, the same matrix also performs the transformation from $\vu{r}\,, \vu{\theta}\,, \vu{\phi}$ to $\vu{k}, \vu{i}, \vu{j}$\footnote{Let $S$ be the matrix in \ref{eq:orthomatrix}. If that is a transformation of a \textit{covariant form} from spherical to Cartesian coordinates, then the same matrix $S$ also performs the transformation of the spherical basis to the Cartesian basis, hence the vectors $\vu{r}\,, \vu{\theta}\,, \vu{\phi}$ of the spherical basis are obtained from the vectors $\vu{k}, \vu{i}, \vu{j}$ of the Cartesian basis by the \textit{inverse} of $S$ which, being $S$ an orthogonal matrix coincides with its \textit{transpose} $S^\top$. Alternatively, if \ref{eq:orthomatrix} had been the transformation of coordinates (from spherical to Cartesian) of a \textit{contravariant vector}, there would be two transpositions to be taken, which is the same as no transposition.} therefore 


\begin{equation}
\label{eq:spherical_differentials_xform}
\mqty[dr \\ r d\theta \\ r \sin(\theta)d\phi]  = \mqty|
\cos(\theta) & \sin(\theta)\cos(\phi) & \sin(\theta)\sin(\phi) \\
-\sin(\theta) & \cos(\theta)\cos(\phi) & \cos(\theta)\sin(\phi) \\
0 &  -\sin(\phi) & \cos(\phi)
| \:  \mqty[dz \\ dx \\ dy] 
\end{equation}

and 

\begin{subequations}
\label{eq:spherical_bases_xform}
\begin{align}
\mqty[\vu{r} \\ \vu{\theta} \\ \vu{\phi}]  = \mqty|
\cos(\theta) & \sin(\theta)\cos(\phi) & \sin(\theta)\sin(\phi) \\
-\sin(\theta) & \cos(\theta)\cos(\phi) & \cos(\theta)\sin(\phi) \\
0 &  -\sin(\phi) & \cos(\phi)
| \:  \mqty[\vu{k} \\ \vu{i} \\ \vu{j}] \\ 
\mqty[\vu{r} \\ \vu{\theta} \\ \vu{\phi}]  = \mqty|
\sin(\theta)\cos(\phi) & \sin(\theta)\sin(\phi) &  \cos(\theta) \\
\cos(\theta)\cos(\phi) & \cos(\theta)\sin(\phi) & -\sin(\theta) \\
-\sin(\phi) & \cos(\phi) & 0 
| \:  \mqty[\vu{i} \\ \vu{j} \\ \vu{k} ]
\end{align}
\end{subequations}



\subsection{Cylindrical Coordinates}
The following equations provide the transformation from cylindrical to Cartesian coordinates: 
  
\section{The Dirac Delta Function}

\subsection{The Divergence of $\vu{r}/r^2$}

\subsection{The Three-Dimensional Delta Function}
    

\section{The Theory of Vector Fields}

\subsection{The Helmoltz Theorem}

\subsection{Potentials}
        